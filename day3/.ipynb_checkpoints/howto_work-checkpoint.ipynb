{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work habits and reproducible research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source your code\n",
    "- Continuous Integration\n",
    "- Reproducible research\n",
    "- Unit Tests\n",
    "- Workflows\n",
    "- Acceleration: profiling, JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"important stuff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(a == \"important stuff\"), \"Ha! You just corrupted your data!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    a = \"corrupted \" + a\n",
    "    return\n",
    "\n",
    "a = f(a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why?**\n",
    "\n",
    "- Because you avoid errors. Because reproducibility. And because you must be a responsible programmer and researcher.\n",
    "- You can version your code with a version control, track changes, assign error reporting, documentation etc.\n",
    "- If the code is kept in Jupyter Notebooks, it should always be executed sequentially.\n",
    "- Did you know, Jupyter has code editor, terminal and markdown editor, independent from the notebook?\n",
    "- Make your research presentation easily reproducible by offering the data, the code and the findings separately.\n",
    "- But, don't isolate the code from your findings! You can execute your source code from a notebook, that is fine!\n",
    "- The bits of notebook that you see on internet are meant to demonstrate code, not do scientific research.\n",
    "- If you receive a notebook from a colaborator, always make sure you can run it and reproduce the findings in it (it may already be corrupted).\n",
    "\n",
    "**How**\n",
    "\n",
    "Python editors:\n",
    "- Simple text processors: Atom, Sublime, Geany, Notepad++ etc\n",
    "- Spyder: good for basic scientific programming, ipython interpreter\n",
    "- PyCharm: refactoring, internal jupyter, object browsing, etc\n",
    "What matters:\n",
    "- Using the editor most appropriate to the complexity of the task.\n",
    "- Full feature editors make it easier to write good code!\n",
    "- Syntax and style linting.\n",
    "- Code refactoring.\n",
    "- Git/svn integration.\n",
    "- Remote development.\n",
    "- Advanced debugging.\n",
    "- Unit testing.\n",
    "\n",
    "\n",
    "**Standards**\n",
    "\n",
    "- Source code can be one or several scripts, it should contain information on deployment, testing and documentation.\n",
    "- Some care for design should be given. Module hierarchy, will you create classes, use of design patterns.\n",
    "    - https://www.geeksforgeeks.org/python-design-patterns/\n",
    "- Python style guide.\n",
    "    - https://www.python.org/dev/peps/pep-0008/\n",
    "\n",
    "\n",
    "**Quality of life, or simply good habits**\n",
    "- Software versioning milestones.\n",
    "- Continuous integration.\n",
    "- Reproducibility.\n",
    "- Workflows.\n",
    "- Containerization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Versioning example**\n",
    "\n",
    "- https://www.python.org/dev/peps/pep-0440/\n",
    "- https://en.wikipedia.org/wiki/Software_versioning\n",
    "\n",
    "milestone x.x:\n",
    "- expected outcomes\n",
    "- tests\n",
    "\n",
    "```\n",
    "X.YaN   # Alpha release\n",
    "X.YbN   # Beta release\n",
    "X.YrcN  # Release Candidate\n",
    "X.Y     # Final release\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Integration\n",
    "\n",
    "- Submit your code to github often!\n",
    "- Make backup for your data and findings.\n",
    "- Set baselines on expected outcomes, and verify often that your code is tested against them.\n",
    "- Unit tests are one way to do this.\n",
    "- Notebook keeping helps (internal notebooks) especially if you can re-test.\n",
    "- Test your code more than once, and everytime you do a modification.\n",
    "- Use workflows, virtual environments and containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible research\n",
    "\n",
    "\n",
    "- The vast majority of published results today are not reproducible.\n",
    "- Let us admit this, if our research cannot pe reproduced we probably did something else.\n",
    "- Research findings do not only depend on your plotting skill\n",
    "- For someone to be able to produce your results several thinks must harmonize:\n",
    "    - Open data access:\n",
    "        - (on federated databases)\n",
    "    - Open source access\n",
    "        - on github, gitlab, etc\n",
    "    - Open environment:\n",
    "        - conda requirements and container script (or image)\n",
    "    - Findings (paper AND notebooks)\n",
    "        - public access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Development vs production**\n",
    "\n",
    "- They are separated, fully.\n",
    "- If is fine to demo your source code, or just parts of it on a notebook during development.\n",
    "- Most notebooks you see on the web are in a development stage.\n",
    "- How does it impact on the reproducibility if the development and production is not separated?\n",
    "- Bring forward the issue of having different projects using the same source code directory, or the same data directory. What is to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducible environments: containers and conda\n",
    "\n",
    "- **Docker usage is described in another notebook**\n",
    "- containers can isolate an environment even better than a package manager!\n",
    "- Problem: what if the old package versions cannot be maintained?\n",
    "- Problem: what if the older container instances cannot be spinned or even re-created?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# export an environment into a yaml file\n",
    "conda environment_name export > environment.yaml\n",
    "# re-create an environment based on the file\n",
    "conda env create -f environment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit testing\n",
    "\n",
    "- The unittest module can be used from the command line to run tests from modules, classes or even individual test methods\n",
    "    - https://docs.python.org/3/library/unittest.html\n",
    "    - https://www.geeksforgeeks.org/unit-testing-python-unittest/\n",
    "    - https://realpython.com/python-testing/\n",
    "- Some editors give special support for unit tests:\n",
    "    - https://www.jetbrains.com/help/pycharm/testing-your-first-python-application.html#choose-test-runner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "\n",
    "- Docstrings convention\n",
    "    - https://www.python.org/dev/peps/pep-0257/\n",
    "    - https://realpython.com/documenting-python-code/\n",
    "- https://readthedocs.org/\n",
    "    - simplifies software documentation by building, versioning, and hosting of your docs, automatically. Think of it as Continuous Documentation\n",
    "- Using Sphinx:\n",
    "    - https://docs.readthedocs.io/en/latest/intro/getting-started-with-sphinx.html\n",
    "    - https://www.sphinx-doc.org/en/master/\n",
    "- Other options exist, pydoc, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Other development tools:**\n",
    "- debugger: allows to to follow your code step by step and investigate the program stack\n",
    "- profiler: shows memory usage finding possible leaks and bottlenecks (see acceleration notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflows\n",
    "\n",
    "**Snakemake**\n",
    "- https://snakemake.readthedocs.io/en/stable/tutorial/basics.html\n",
    "- https://snakemake.github.io/snakemake-workflow-catalog/\n",
    "\n",
    "```\n",
    "conda install -c bioconda snakemake\n",
    "conda install graphviz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "SAMPLES = ['ctl1', 'ctl2']\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        'merged.txt'\n",
    "\n",
    "rule acounts:\n",
    "    input:\n",
    "        file='{sample}.fastq'\n",
    "    output:\n",
    "        '{sample}_counts.txt'\n",
    "    run:\n",
    "        with open(input.file, 'r') as f:\n",
    "            nc = [str(l.count('A')) for l in f if not l[0]=='@']\n",
    "        data = ', '.join(nc)+'\\n'\n",
    "        with open(output[0], 'w') as f: f.write(data)\n",
    "\n",
    "rule merge:\n",
    "    input:\n",
    "        counts=expand('{sample}_counts.txt',sample=SAMPLES)\n",
    "    output:\n",
    "        'merged.txt'\n",
    "    shell:\n",
    "        \"\"\"\n",
    "        for f in {input.counts}\n",
    "        do\n",
    "\t\t\tcat $f >> {output}\n",
    "\t\tdone\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "snakemake --dag merged.txt | dot -Tsvg > dag.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning.ipynb\tscicomp.ipynb\t  visualization.ipynb\n",
      "networks.ipynb\tstatistics.ipynb  workflows.ipynb\n"
     ]
    }
   ],
   "source": [
    "snakemake --name mylittleworkflow.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nextflow**\n",
    "- https://www.nextflow.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#!/usr/bin/env nextflow\n",
    " \n",
    "params.range = 100\n",
    " \n",
    "/*\n",
    " * A trivial Perl script producing a list of numbers pair\n",
    " */\n",
    "process perlTask {\n",
    "    output:\n",
    "    stdout randNums\n",
    " \n",
    "    shell:\n",
    "    '''\n",
    "    #!/usr/bin/env perl\n",
    "    use strict;\n",
    "    use warnings;\n",
    " \n",
    "    my $count;\n",
    "    my $range = !{params.range};\n",
    "    for ($count = 0; $count < 10; $count++) {\n",
    "        print rand($range) . ', ' . rand($range) . \"\\n\";\n",
    "    }\n",
    "    '''\n",
    "}\n",
    " \n",
    " \n",
    "/*\n",
    " * A Python script task which parses the output of the previous script\n",
    " */\n",
    "process pyTask {\n",
    "    echo true\n",
    " \n",
    "    input:\n",
    "    stdin randNums\n",
    " \n",
    "    '''\n",
    "    #!/usr/bin/env python\n",
    "    import sys\n",
    " \n",
    "    x = 0\n",
    "    y = 0\n",
    "    lines = 0\n",
    "    for line in sys.stdin:\n",
    "        items = line.strip().split(\",\")\n",
    "        x = x+ float(items[0])\n",
    "        y = y+ float(items[1])\n",
    "        lines = lines+1\n",
    " \n",
    "    print \"avg: %s - %s\" % ( x/lines, y/lines )\n",
    "    '''\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed: Profiling, IPython, JIT\n",
    "\n",
    "The Python standard library contains the cProfile module for determining the time that takes every Python function when running the code. The pstats module allows to read the profiling results. Third party profiling libraries include in particular line_profiler for profiling code line after line, and memory_profiler for profiling memory usage. All these tools are very powerful and extremely useful when optimizing some code, but they might not be very easy to use at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.py\n",
    "import numpy as np\n",
    "import numpy.random as rdn\n",
    "\n",
    "# uncomment for line_profiler\n",
    "# @profile\n",
    "def test():\n",
    "    a = rdn.randn(100000)\n",
    "    b = np.repeat(a, 100)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m cProfile -o prof script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "$ pip install ipython\n",
    "$ ipython --version\n",
    "0.13.1\n",
    "$ pip install line-profiler\n",
    "$ pip install psutil\n",
    "$ pip install memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%timeit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run -t slow_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 299 ms\n",
      "7.37 ns ± 0.0881 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%time {1 for i in range(10*1000000)}\n",
    "%timeit -n 1000 10*1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def foo(n):\n",
    "    phrase = 'repeat me'\n",
    "    pmul = phrase * n\n",
    "    pjoi = ''.join([phrase for x in xrange(n)])\n",
    "    pinc = ''\n",
    "    for x in xrange(n):\n",
    "        pinc += phrase\n",
    "    del pmul, pjoi, pinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext line_profiler\n",
    "%lprun -f foo foo(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- %time & %timeit: See how long a script takes to run (one time, or averaged over a bunch of runs).\n",
    "- %prun: See how long it took each function in a script to run.\n",
    "- %lprun: See how long it took each line in a function to run.\n",
    "- %mprun & %memit: See how much memory a script uses (line-by-line, or averaged over a bunch of runs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba is an open source JIT (just in time) compiler that translates a subset of Python and NumPy code into fast machine code.\n",
    "- https://numba.pydata.org/\n",
    "\n",
    "```\n",
    "conda install numba\n",
    "conda install cudatoolkit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import random\n",
    "\n",
    "@jit(nopython=True)\n",
    "def monte_carlo_pi(nsamples):\n",
    "    acc = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x ** 2 + y ** 2) < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / nsamples\n",
    "\n",
    "@numba.jit(nopython=True, parallel=True)\n",
    "def logistic_regression(Y, X, w, iterations):\n",
    "    for i in range(iterations):\n",
    "        w -= np.dot(((1.0 /\n",
    "              (1.0 + np.exp(-Y * np.dot(X, w)))\n",
    "              - 1.0) * Y), X)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JIT with JAX\n",
    "Jax is a JIT compiler optimized for machine learning.\n",
    "- https://github.com/google/jax\n",
    "- https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
