{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/big-data-on-amazon-elastic-mapreduce/run-a-spark-job-within-amazon-emr-in-15-minutes-68b02af1ae16\n",
    "\n",
    "https://www.themarketingtechnologist.co/upload-your-local-spark-script-to-an-aws-emr-cluster-using-a-simply-python-script/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8888/lab\n",
    "https://keep.google.com/u/0/#search/text%253Dspark\n",
    "https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873\n",
    "https://medium.com/big-data-on-amazon-elastic-mapreduce/run-a-spark-job-within-amazon-emr-in-15-minutes-68b02af1ae16\n",
    "https://becominghuman.ai/real-world-python-workloads-on-spark-standalone-clusters-2246346c7040\n",
    "https://towardsdatascience.com/how-to-get-started-with-pyspark-1adc142456ec\n",
    "https://www.google.com/search?q=spark+on+amazon+ec2&oq=spark+on+amazon&aqs=chrome.3.69i57j0l5.11481j0j1&sourceid=chrome&ie=UTF-8\n",
    "https://spark.apache.org/docs/1.6.2/ec2-scripts.html\n",
    "https://aws.amazon.com/big-data/what-is-spark/\n",
    "https://eu-north-1.console.aws.amazon.com/console/home?region=eu-north-1#\n",
    "https://eu-north-1.console.aws.amazon.com/elasticmapreduce/home?region=eu-north-1#\n",
    "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan.html\n",
    "https://stackoverflow.com/questions/38611573/how-to-launch-spark-2-0-on-ec2\n",
    "https://www.google.com/search?q=simple+tutorial+for+running+spark+in+aws&oq=simple+tutorial+for+running+spark+in+aws&aqs=chrome..69i57j33.14978j0j1&sourceid=chrome&ie=UTF-8\n",
    "https://advpy2019.slack.com/messages/DJSPA69AL/?\n",
    "https://www.google.com/search?q=python+running+spark+on+ec2&oq=python+running+spark+on+ec2&aqs=chrome..69i57j33l3.9566j0j1&sourceid=chrome&ie=UTF-8\n",
    "https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297\n",
    "https://towardsdatascience.com/clean-up-your-own-model-data-without-leaving-jupyter-bdbcc9001734\n",
    "https://medium.com/jbennetcodes/how-to-get-rid-of-loops-and-use-window-functions-in-pandas-or-spark-sql-907f274850e4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ conda install -c conda-forge awscli\n",
    "$ aws --version\n",
    "aws-cli/1.16.161 Python/3.7.3 Linux/4.18.0-20-generic botocore/1.12.151\n",
    "\n",
    "```\n",
    "\n",
    "Then go to EMR and pick the cheapest option for your region, optimized for computing, mine is c5.xlarge.\n",
    "- https://aws.amazon.com/emr/pricing/?nc1=h_ls\n",
    "- https://aws.amazon.com/s3/pricing/\n",
    "\n",
    "The region settings for Stockholm can be found at:\n",
    "https://docs.aws.amazon.com/general/latest/gr/rande.html#apigateway_region\n",
    "I am using the Stockholm region (eu-north-1).\n",
    "\n",
    "Next you need access keys for IAM roles. Open https://console.aws.amazon.com/iam/ and folow the instructions [here][1].\n",
    "\n",
    "```\n",
    "$ aws configure\n",
    "AWS Access Key ID [None]: AKIAYGDZBRYTTYE5Y2N6\n",
    "AWS Secret Access Key [None]:....\n",
    "Default region name [None]: eu-north-1\n",
    "Default output format [None]: json\n",
    "\n",
    "$ aws emr create-default-roles\n",
    "\n",
    "$ aws ec2 describe-subnets \\\n",
    ">      --filters \"Name=availabilityZone,Values=eu-north-1\"\n",
    "{\n",
    "    \"Subnets\": []\n",
    "}\n",
    "\n",
    "aws emr add-steps --cluster-id <your-cluster-job-id> --steps Name=Python Job,Jar=http://s3://elasticmapreduce/libs/script-runner/script-runner.jar,Args=[/home/hadoop/spark/bin/spark-submit,--deploy-mode,cluster,--master,yarn,s3://my_bucket/path/pythonjob.py,<comma separated list of arguments for your app>],ActionOnFailure=CONTINUE\n",
    "\n",
    "```\n",
    "\n",
    "Now \n",
    "\n",
    "\n",
    "aws emr create-cluster \\\n",
    "--name \"sparkclust\" \\\n",
    "--release-label emr-5.23.0 \\\n",
    "--applications Name=Hadoop Name=Spark \\\n",
    "--ec2-attributes KeyName=spark_keypair \\\n",
    "--instance-groups \\\n",
    "Name=EmrMaster,InstanceGroupType=MASTER,InstanceCount=1,InstanceType=c5.xlarge \\\n",
    "Name=EmrCore,InstanceGroupType=CORE,InstanceCount=2,InstanceType=c5.xlarge \\\n",
    "--use-default-roles\n",
    "\n",
    "aws emr ssh --cluster-id j-3H0XTRI8687P2 --key-pair-file /home/sergiu/Downloads/spark_keypair.pem\n",
    "\n",
    "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-connect-master-node-ssh.html\n",
    "\n",
    "$ aws emr list-clusters\n",
    "$ aws emr describe-cluster --cluster-id j-3H0XTRI8687P2\n",
    "# look for the \"MasterPublicDnsName\": \"ec2-13-48-55-199.eu-north-1.compute.amazonaws.com\"\n",
    "\n",
    "Before you can ssh to the master node you have to enable SSH to \"My IP\" (your computer's IP) on both the master and slave subnets.\n",
    "(visit Summary: Security groups for Master)\n",
    "\n",
    "ssh hadoop@ec2-13-48-55-199.eu-north-1.compute.amazonaws.com -i /home/sergiu/Downloads/spark_keypair.pem\n",
    "\n",
    "\n",
    "[1]: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used from https://github.com/apache/spark/blob/master/examples/src/main/python/pi.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "        Usage: pi [partitions]\n",
    "    \"\"\"\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPi\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
    "    n = 100000 * partitions\n",
    "\n",
    "    def f(_):\n",
    "        x = random() * 2 - 1\n",
    "        y = random() * 2 - 1\n",
    "        return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "    count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run cluster jobs interactively by SSH to teh master node. But there are many other ways in which you can run your jobs on the spark cluster. Complete freedom!\n",
    "\n",
    "```\n",
    "$ ls /usr/lib/spark/python/lib/\n",
    "py4j-0.10.7-src.zip  PY4J_LICENSE.txt  py4j-src.zip  pyspark.zip\n",
    "[hadoop@ip-172-31-16-184 ~]$ export SPARK_HOME=/usr/lib/spark\n",
    "[hadoop@ip-172-31-16-184 ~]$ export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH\n",
    "[hadoop@ip-172-31-16-184 ~]$ export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/build:$PYTHONPATH\n",
    "[hadoop@ip-172-31-16-184 ~]$ source ~/.bashrc\n",
    "\n",
    "$ python pitest.py \n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "19/05/18 17:30:03 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
    "[Stage 0:>                                                          (0 + 0) / 2]19/05/18 17:30:19 WARN TaskSetManager: Stage 0 contains a task of very large size (371 KB). The maximum recommended task size is 100 KB.\n",
    "Pi is roughly 3.142540        \n",
    "```\n",
    "\n",
    "You can also add steps via the awscli and monitor them online:\n",
    "\n",
    "```\n",
    "$ aws emr add-steps \\\n",
    "--cluster-id j-3H0XTRI8687P2 \\\n",
    "--steps Type=CUSTOM_JAR,Name=\"Spark Program\",Jar=\"command-runner.jar\",ActionOnFailure=CONTINUE,Args=[\"spark-submit\",/home/hadoop/pitest.py]\n",
    "```\n",
    "\n",
    "## SUPER IMPORTANT STEP..\n",
    "\n",
    "```\n",
    "aws emr terminate-clusters --cluster-ids j-3H0XTRI8687P2\n",
    "```\n",
    "\n",
    "***OBS: Verify on https://eu-north-1.console.aws.amazon.com/elasticmapreduce/ that your cluster was terminated properly!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "- Log the result in an s3 bucket.\n",
    "- Try to load a data set and perform a basic ML task!\n",
    "- Configure JupyterHub via EMR, load giant pyspark steps from the comfort of your phone's web browser. Profit! ;)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8888/lab\n",
    "https://keep.google.com/u/0/#search/text%253Dspark\n",
    "https://www.themarketingtechnologist.co/upload-your-local-spark-script-to-an-aws-emr-cluster-using-a-simply-python-script/\n",
    "http://queirozf.com/entries/using-command-line-tools-to-manage-spark-clusters-on-emr-examples-and-reference\n",
    "https://medium.com/big-data-on-amazon-elastic-mapreduce/run-a-spark-job-within-amazon-emr-in-15-minutes-68b02af1ae16\n",
    "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\n",
    "https://console.aws.amazon.com/iam/home?#/users/sergiun?section=security_credentials\n",
    "https://www.quora.com/How-do-you-automate-pyspark-jobs-on-AWS-EMR\n",
    "http://spark.apache.org/docs/latest/submitting-applications.html\n",
    "https://docs.aws.amazon.com/general/latest/gr/rande.html#apigateway_region\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
