{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon web services (AWS)\n",
    "\n",
    "- [Loading data into S3 buckets](#Loading-data-into-S3-buckets)\n",
    "    - via Console, CLI, Boto3\n",
    "- [Setting up an EC2 reserved instance](#Setting-up-a-reserved-instance)\n",
    "    - via Console, CLI, Boto3\n",
    "- [Spin up containers via Docker Machine](#Spin-up-containers-via-Docker-Machine)\n",
    "- [Instance types](#Instance-types)\n",
    "- [ECS clusters and Docker Cloud](#ECS-clusters-and-Docker-Cloud)\n",
    "\n",
    "TODO:\n",
    "- (Make task) Getting Spark Python and Jupyter notebook running on Amazon EC2\n",
    "- https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "There is not special reason to study AWS compared to Google Cloud, Azure, Digital Ocean etc. Amazon Cloud is probably the most popular today, so it offers a nice parralel to Python. AWS is the web-based gateway to the Amazon Cloud computing resources.\n",
    "\n",
    "Of note, AWS [will deploy a region in Sweden](https://aws.amazon.com/blogs/aws/coming-in-2018-new-aws-region-in-sweden/) this year, which will make it interesting for genomics research, especially since it will be made GDPR compliant. Currently no Swedish patient data can be processed on premises outside of Sweden, but the cloud is a player in general non-clinical research.\n",
    "\n",
    "[AWS](https://aws.amazon.com/) is an umbrella for a large number of computing resources, starting from storage and ending with the management of the remote computing infrastructure. To be practical, our focus is on loading data into a bucket, setting up a cloud instance, and later using Docker to remotely spin up cloud instances. We will also learn how to manage these resources via Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into S3 buckets\n",
    "\n",
    "Let us start with loading data. This is a common operation when you want to share your research result with someone, but it can also be useful for yourself as a way to backup your data. Clouds use the concept of 'buckets' to hold data. The 'objects' stored in a bucket can have any encoding, from text to film. There used to be severe penalties on loading super massive objects. Today however, the maximum size for an object is 5TB (on AWS).\n",
    "\n",
    "We will learn how to do this via the web console, via the command line interface and via Python. Note that even thogh these options seem like separated, they are actually using the same API.\n",
    "\n",
    "### Web Console\n",
    "\n",
    "Task:\n",
    "- Use the console to load a test file onto a S3 bucket\n",
    "- Follow this doc link: https://docs.aws.amazon.com/AmazonS3/latest/user-guide/upload-objects.html\n",
    "- Use the following shell command to generate some test data, or use your own:\n",
    "```\n",
    "$ for i in {1..5}; do echo \"l$i\">f$i.txt && gzip f$i.txt; done && \\\n",
    "zcat f*.txt.gz| gzip > f.gz\n",
    "```\n",
    "- Figure out how much your bucket would cost (tip: it is free up to a threshold)!\n",
    "\n",
    "### Amazon CLI\n",
    "\n",
    "Now let's repeat those steps using the command line interface. But first, we must install it.\n",
    "\n",
    "Links: \n",
    "- https://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html\n",
    "- https://aws.amazon.com/getting-started/tutorials/backup-to-s3-cli/\n",
    "\n",
    "```\n",
    "$ sudo apt install awscli\n",
    "$ aws configure\n",
    "AWS Access Key ID [None]: \n",
    "AWS Secret Access Key [None]:\n",
    "(also used eu-central-1 for region, and json as format)\n",
    "```\n",
    "\n",
    "The above commang needs SSL certificates. To generate the aws keys:\n",
    "- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-up-ami-tools.html?icmpid=docs_iam_console#ami-tools-managing-certs\n",
    "\n",
    "```\n",
    "$ openssl genrsa 2048 > aws-private.pem\n",
    "$ openssl req -new -x509 -nodes -sha256 -days 365 -key aws-private.pem -outform PEM -out aws-certificate.pem\n",
    "\n",
    "# if in dire need for security use:\n",
    "$ sudo apt-get install xclip\n",
    "$ xclip -sel clip < ~/.ssh/aws-private.pem\n",
    "```\n",
    "\n",
    "Now that you installed the CLI, here are the main bucket related activities:\n",
    "\n",
    "```\n",
    "aws s3 mb s3://my-first-backup-bucket\n",
    "upload:\n",
    "aws s3 cp “C:\\users\\my first backup.bak” s3://my-first-backup-bucket/\n",
    "download:\n",
    "aws s3 cp s3://my-first-backup-bucket/my-first-backup.bak ./\n",
    "delete:\n",
    "aws s3 rm s3://my-first-backup-bucket/my-first-backup.bak\n",
    "```\n",
    "\n",
    "Data can also be streamed towards a bucket. This can be useful to avoid unnecesary space waste onto the local cloud or PC, but it can be just as useful when it comes to using bucket data without storing all that data locally. It can be done via piping, or proccess substitution:\n",
    "\n",
    "```\n",
    "$ aws s3 mb s3://siofuysni78\n",
    "$ zcat f*.txt.gz| gzip | aws s3 cp - s3://siofuysni78/f.gz\n",
    "$ aws s3 rm s3://siofuysni78/f.gz\n",
    "$ aws s3 rb s3://siofuysni78 --force\n",
    "```\n",
    "\n",
    "Why did I use such a weird name? It is because Amazon indexes all buckets by their name, thus a name such as \"test123\" will never fly. Here is how to stream from S3 to your computing resource (it can be a cloud instance, you local machine or a remore server)\n",
    "\n",
    "```\n",
    "$ aws s3 mb s3://siofuysni78\n",
    "$ zcat f*.txt.gz| gzip | aws s3 cp - s3://siofuysni78/f.gz\n",
    "$ aws s3 cp s3://siofuysni78/f.gz - | gunzip | grep 1\n",
    "l1\n",
    "```\n",
    "\n",
    "### Boto3\n",
    "\n",
    "\n",
    "Links:\n",
    "- http://boto3.readthedocs.io/en/latest/guide/migration.html#installation-configuration\n",
    "- https://boto3.readthedocs.io/en/latest/guide/s3-example-creating-buckets.html\n",
    "- http://boto3.readthedocs.io/en/latest/reference/services/s3.html\n",
    "\n",
    "\n",
    "```\n",
    "conda install -c anaconda boto3\n",
    "pip install boto3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket List: ['crasstestdummy', 'jo8a7fn8sfn8', 'siofuysni78', 'snlmocombined']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# initialize the S3 service\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# create a test bucket (tip: use a different name!)\n",
    "s3.create_bucket(Bucket='jo8a7fn8sfn8', CreateBucketConfiguration={'LocationConstraint': 'eu-central-1'})\n",
    "\n",
    "# Call S3 to list current buckets\n",
    "response = s3.list_buckets()\n",
    "\n",
    "# Get a list of all bucket names from the response\n",
    "buckets = [bucket['Name'] for bucket in response['Buckets']]\n",
    "\n",
    "# Print out the bucket list\n",
    "print(\"Bucket List: %s\" % buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "filename = '/path/to/test/file'\n",
    "bucket_name = 'jo8a7fn8sfn8'\n",
    "\n",
    "# Uploads the given file using a managed uploader, which will split up large\n",
    "# files automatically and upload parts in parallel.\n",
    "s3.upload_file(filename, bucket_name, filename)\n",
    "\n",
    "# or\n",
    "# s3.Object('mybucket', 'hello.txt').put(Body=open('/tmp/hello.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://boto3.readthedocs.io/en/latest/guide/migrations3.html#deleting-a-bucket\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('jo8a7fn8sfn8')\n",
    "\n",
    "for key in bucket.objects.all():\n",
    "    key.delete()\n",
    "bucket.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Now I want to test using the buchet without local file storage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a reserved instance\n",
    "\n",
    "Amazon names their most popular instances Elastic Compute Cloud (EC2).\n",
    "- https://aws.amazon.com/ec2/\n",
    "- https://docs.aws.amazon.com/efs/latest/ug/gs-step-one-create-ec2-resources.html\n",
    "- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html\n",
    "\n",
    "Probably the most basic level of access to the Amazon computing infrastructure is setting up a free tier reserved instance. \n",
    "\n",
    "### Web Console\n",
    "\n",
    "Task:\n",
    "- Setup an AWS instance using the Free Tier (don't forget to close it!).\n",
    "- [https://aws.amazon.com/console/](https://aws.amazon.com/console/)\n",
    "\n",
    "\n",
    "### Amazon CLI\n",
    "\n",
    "\n",
    "```\n",
    "aws ec2 run-instances --image-id ami-xxxxxxxx --count 1 --instance-type t1.micro --key-name MyKeyPair --security-groups my-sg\n",
    "```\n",
    "\n",
    "    \n",
    "### Boto3\n",
    "\n",
    "\n",
    "- http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#instance\n",
    "- http://boto3.readthedocs.io/en/latest/guide/migrationec2.html#launching-new-instances\n",
    "\n",
    "Task:\n",
    "- A larger task is to create an instance with Boto3, install an SSH client such as Paramaiko and run commands on the remote client.\n",
    "\n",
    "Helpful code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import paramiko\n",
    "\n",
    "\n",
    "ec2 = boto3.resource('ec2')\n",
    "instance = ec2.Instance('id')\n",
    "ec2.create_instances(ImageId='<ami-image-id>', MinCount=1, MaxCount=5)\n",
    "\n",
    "key = paramiko.RSAKey.from_private_key_file(path/to/mykey.pem)\n",
    "client = paramiko.SSHClient()\n",
    "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "# Connect/ssh to an instance\n",
    "try:\n",
    "    # Here 'ubuntu' is user name and 'instance_ip' is public IP of EC2\n",
    "    client.connect(hostname=instance_ip, username=\"ubuntu\", pkey=key)\n",
    "\n",
    "    # Execute a command(cmd) after connecting/ssh to an instance\n",
    "    stdin, stdout, stderr = client.exec_command(cmd)\n",
    "    print stdout.read()\n",
    "\n",
    "    # close the client connection once the job is done\n",
    "    client.close()\n",
    "    break\n",
    "\n",
    "except Exception, e:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin up containers via Docker Machine\n",
    "\n",
    "My preferred way is to use docker machine in order to manage cloud instances that are already set-up with Docker. Then you can pull your intended container from the Docker Hub and run it on the instance. An alternative is usign AWS services to create your instance, which has its own benefits (basically most benefits except for time). Another alternative is usign Docker Cloud or Kubernetes, which is the way to go for multiple instances.\n",
    "\n",
    "\n",
    "```\n",
    "# install docker machine\n",
    "$ base=https://github.com/docker/machine/releases/download/v0.14.0 && \\\n",
    "curl -L $base/docker-machine-$(uname -s)-$(uname -m) >/tmp/docker-machine && \\\n",
    "sudo install /tmp/docker-machine /usr/local/bin/docker-machine\n",
    "\n",
    "# setup a cloud instance\n",
    "$ export SECRET_KEY=\"...\"\n",
    "$ docker-machine create --driver amazonec2 --amazonec2-region eu-central-1 \\\n",
    "--amazonec2-access-key AKIAJPBEKSXQ7NJGSL3A \\\n",
    "--amazonec2-secret-key $SECRET_KEY \\\n",
    "aws-test\n",
    "\n",
    "# ssh and delete\n",
    "docker-machine ssh aws-test\n",
    "docker-machine rm aws-test\n",
    "\n",
    "# for other options: --amazonec2-instance-type \"t2.2xlarge\"\n",
    "docker-machine create --driver amazonec2\n",
    "```\n",
    "\n",
    "\n",
    "**Further read**\n",
    "\n",
    "- On Docker Machine:\n",
    "    - https://docs.docker.com/machine/examples/aws/\n",
    "    - https://docker-curriculum.com/\n",
    "    - https://sreeninet.wordpress.com/2016/09/03/docker-machine-for-aws/\n",
    "    - http://blog.wimwauters.com/docker-getting-started-with-docker-machine-aws/\n",
    "    - http://docker-k8s-lab.readthedocs.io/en/latest/docker/docker-machine-aws.html\n",
    "- Using AWS alternative:\n",
    "    - https://aws.amazon.com/getting-started/tutorials/deploy-docker-containers/\n",
    "    - https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html#docker-basics-create-image\n",
    "    - https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\n",
    "- Or the more simplified use of Docker cloud, by linking to AWS. There are other, such as Kubernetes, but it is too complex (we only need one instance):\n",
    "    - https://docs.docker.com/docker-cloud/infrastructure/link-aws/\n",
    "    - https://docs.docker.com/docker-cloud/getting-started/your_first_node/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull the docker hub container on the EC2 instance, open shell and run test\n",
    "\n",
    "```\n",
    "# run inside the EC2 instance\n",
    "export DOCKER_ID_USER=\"grokkaine\"\n",
    "docker login\n",
    "docker pull $DOCKER_ID_USER/awstest\n",
    "docker run -ti $DOCKER_ID_USER/awstest /bin/bash\n",
    "\n",
    "# now run your commands inside the container\n",
    "```\n",
    "\n",
    "This works only if you must run short tasks, because once you log out from the container, the container will end. What you need is to be able to run long jobs. So we must create a detached container, then attach to it during and after the execution of the program with a shell in order to check the logs and save data.\n",
    "\n",
    "```\n",
    "# run inside the EC2 instance\n",
    "export DOCKER_ID_USER=\"\"\n",
    "sudo docker login\n",
    "sudo docker pull $DOCKER_ID_USER/awscrass\n",
    "sudo docker run -w /home/ -tid $DOCKER_ID_USER/awscrass /bin/bash\n",
    "\n",
    "# exit container, start it\n",
    "sudo docker ps\n",
    "# run a command in detached mode\n",
    "#sudo docker exec -d containerid bash -c \"your command line\"\n",
    "\n",
    "#alternative is to log into the container and run the command there\n",
    "sudo docker exec -it containerid bash\n",
    "\n",
    "# start, attach\n",
    "docker start containerid\n",
    "docker attach containerid\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pricing\n",
    "\n",
    "- storage on S3 buckets:\n",
    "150 GB * 0.022$ /month = 3$\n",
    "- transfer:\n",
    "150 GB * 0.09$ /month = 13$\n",
    "- compute using m5.4xlarge (64GiB RAM) on demand instances:\n",
    "20 days * 0.9$/hour = 432$\n",
    "- compute using t2.2xlarge (32GiB RAM)\n",
    "20 days * 0.42$/hour = 201$\n",
    "\n",
    "Further read:\n",
    "\n",
    "- autoscaling? https://aws.amazon.com/autoscaling/\n",
    "- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html\n",
    "- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html\n",
    "- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "\n",
    "Your task will be to create a Docker container, push it to Docker Hub, start an EC2 instance and remotely run your container, log out from the instance, then logging back and checking that the output is preserved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Types\n",
    "\n",
    "On AWS you can opt for different types of instances, and you can also upgrade or downgrade your instance to meet your need for resources. One can opt for example for instances that have a lot of RAM assigned when using a RAM intensive computation such as sequence alignment, or GPU instances when needing deep learning or other forms of GPU accelerated computing. More here: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html\n",
    "\n",
    "From the purchasing point of view there are however several major classes of instances, most notably:\n",
    "- On demand instances. Such instances are available when you request them and will be held up until you close them. - Spot instances. A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. \n",
    "- read about the other types here: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECS clusters and Docker Cloud\n",
    "\n",
    "\n",
    "You can run containerized clusters of EC2 instances using another AWS web service called ECS clusters. More information here: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_clusters.html\n",
    "\n",
    "Another popular option is to use Docker Cloud, allowing you to define and store images and build mechanisms of continuous integration then run tests or production clusters on AWS.\n",
    "\n",
    "Task:\n",
    "- Learn how to deply an Elastic HPC cluster: https://aws.amazon.com/getting-started/projects/deploy-elastic-hpc-cluster/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
