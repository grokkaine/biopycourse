{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud computing\n",
    "- AWS access: Console, CLI, Boto3\n",
    "- loading data into buckets\n",
    "- setting up a reserved instance\n",
    "- using containers\n",
    "- distributed computing via pyspark\n",
    "- run jupyter on ec2 instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon web services (AWS)\n",
    "\n",
    "- No special reason to study Amazon Cloud compared to Google Cloud, Azure, Digital Ocean etc.\n",
    "- Amazon Cloud is the most popular cloud today. \n",
    "- AWS is the web-based gateway to the Amazon Cloud computing resources.\n",
    "- AWS has a region in Sweden\n",
    "- AWS is an umbrella for a large number of computing resources, starting from storage and ending with the management of the remote computing infrastructure.\n",
    "- https://aws.amazon.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data into S3 buckets\n",
    "\n",
    "\n",
    "- Useful for: backup, sharing research or data (when legal)\n",
    "- Clouds use the concept of 'buckets' to hold data. \n",
    "- The 'objects' stored in a bucket can have any encoding, from text to film.\n",
    "- The maximum size for one object is 5TB (on AWS).\n",
    "- Internaly the web console, via the command line interface and Boto3 share the same API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using the web console**\n",
    "\n",
    "Task:\n",
    "- Use the console to load a test file onto a S3 bucket\n",
    "- Follow this doc link: https://docs.aws.amazon.com/AmazonS3/latest/user-guide/upload-objects.html\n",
    "- Use the following shell command to generate some test data, or use your own:\n",
    "```\n",
    "$ for i in {1..5}; do echo \"l$i\">f$i.txt && gzip f$i.txt; done && \\\n",
    "zcat f*.txt.gz| gzip > f.gz\n",
    "```\n",
    "- Figure out how much your bucket would cost (tip: it is free up to a threshold)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using the CLI (command line interface)**\n",
    "\n",
    "Now let's repeat those steps using the command line interface. But first, we must install it.\n",
    "\n",
    "Links: \n",
    "- https://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html\n",
    "- https://aws.amazon.com/getting-started/tutorials/backup-to-s3-cli/\n",
    "\n",
    "```\n",
    "$ sudo apt install awscli\n",
    "$ aws configure\n",
    "AWS Access Key ID [None]: \n",
    "AWS Secret Access Key [None]:\n",
    "(also used eu-central-1 for region, and json as format)\n",
    "```\n",
    "\n",
    "The above commang needs SSL certificates. To generate the aws keys:\n",
    "- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-up-ami-tools.html?icmpid=docs_iam_console#ami-tools-managing-certs\n",
    "\n",
    "```\n",
    "$ openssl genrsa 2048 > aws-private.pem\n",
    "$ openssl req -new -x509 -nodes -sha256 -days 365 -key aws-private.pem -outform PEM -out aws-certificate.pem\n",
    "\n",
    "# if in dire need for security use:\n",
    "$ sudo apt-get install xclip\n",
    "$ xclip -sel clip < ~/.ssh/aws-private.pem\n",
    "```\n",
    "\n",
    "Now that you installed the CLI, here are the main bucket related activities:\n",
    "\n",
    "```\n",
    "aws s3 mb s3://my-first-backup-bucket\n",
    "upload:\n",
    "aws s3 cp “C:\\users\\my first backup.bak” s3://my-first-backup-bucket/\n",
    "download:\n",
    "aws s3 cp s3://my-first-backup-bucket/my-first-backup.bak ./\n",
    "delete:\n",
    "aws s3 rm s3://my-first-backup-bucket/my-first-backup.bak\n",
    "```\n",
    "\n",
    "Data can also be streamed towards a bucket. This can be useful to avoid unnecesary space waste onto the local cloud or PC, but it can be just as useful when it comes to using bucket data without storing all that data locally. It can be done via piping, or proccess substitution:\n",
    "\n",
    "```\n",
    "$ aws s3 mb s3://siofuysni78\n",
    "$ zcat f*.txt.gz| gzip | aws s3 cp - s3://siofuysni78/f.gz\n",
    "$ aws s3 rm s3://siofuysni78/f.gz\n",
    "$ aws s3 rb s3://siofuysni78 --force\n",
    "```\n",
    "\n",
    "Why did I use such a weird name? It is because Amazon indexes all buckets by their name, thus a name such as \"test123\" will never fly. Here is how to stream from S3 to your computing resource (it can be a cloud instance, you local machine or a remore server)\n",
    "\n",
    "```\n",
    "$ aws s3 mb s3://siofuysni78\n",
    "$ zcat f*.txt.gz| gzip | aws s3 cp - s3://siofuysni78/f.gz\n",
    "$ aws s3 cp s3://siofuysni78/f.gz - | gunzip | grep 1\n",
    "l1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using Boto3**\n",
    "\n",
    "\n",
    "Links:\n",
    "- https://aws.amazon.com/sdk-for-python/\n",
    "- https://github.com/boto/boto3\n",
    "- http://boto3.readthedocs.io/en/latest/guide/migration.html#installation-configuration\n",
    "- https://boto3.readthedocs.io/en/latest/guide/s3-example-creating-buckets.html\n",
    "- http://boto3.readthedocs.io/en/latest/reference/services/s3.html\n",
    "\n",
    "\n",
    "```\n",
    "conda install -c anaconda boto3\n",
    "pip install boto3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket List: ['crasstestdummy', 'jo8a7fn8sfn8', 'siofuysni78', 'snlmocombined']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# initialize the S3 service\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# create a test bucket (tip: use a different name!)\n",
    "s3.create_bucket(Bucket='jo8a7fn8sfn8', CreateBucketConfiguration={'LocationConstraint': 'eu-central-1'})\n",
    "\n",
    "# Call S3 to list current buckets\n",
    "response = s3.list_buckets()\n",
    "\n",
    "# Get a list of all bucket names from the response\n",
    "buckets = [bucket['Name'] for bucket in response['Buckets']]\n",
    "\n",
    "# Print out the bucket list\n",
    "print(\"Bucket List: %s\" % buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "filename = '/path/to/test/file'\n",
    "bucket_name = 'jo8a7fn8sfn8'\n",
    "\n",
    "# Uploads the given file using a managed uploader, which will split up large\n",
    "# files automatically and upload parts in parallel.\n",
    "s3.upload_file(filename, bucket_name, filename)\n",
    "\n",
    "# or\n",
    "# s3.Object('mybucket', 'hello.txt').put(Body=open('/tmp/hello.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://boto3.readthedocs.io/en/latest/guide/migrations3.html#deleting-a-bucket\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('jo8a7fn8sfn8')\n",
    "\n",
    "for key in bucket.objects.all():\n",
    "    key.delete()\n",
    "bucket.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Now I want to test using the buchet without local file storage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a reserved instance\n",
    "\n",
    "\n",
    "What can be done on a cloud instance?\n",
    "- It works as a remote PC, on which you have admin priviledge. You can put anything there, run any program. (make sure it is legal)\n",
    "\n",
    "Amazon names their most popular instances Elastic Compute Cloud (EC2).\n",
    "- https://aws.amazon.com/ec2/\n",
    "- https://docs.aws.amazon.com/efs/latest/ug/gs-step-one-create-ec2-resources.html\n",
    "- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html\n",
    "\n",
    "Probably the most basic level of access to the Amazon computing infrastructure is setting up a free tier reserved instance. \n",
    "\n",
    "### Web Console\n",
    "\n",
    "Task:\n",
    "- Setup an AWS instance using the Free Tier (don't forget to close it!).\n",
    "- [https://aws.amazon.com/console/](https://aws.amazon.com/console/)\n",
    "\n",
    "\n",
    "### Amazon CLI\n",
    "\n",
    "\n",
    "```\n",
    "aws ec2 run-instances --image-id ami-xxxxxxxx --count 1 --instance-type t1.micro --key-name MyKeyPair --security-groups my-sg\n",
    "```\n",
    "\n",
    "    \n",
    "### Boto3\n",
    "\n",
    "\n",
    "- http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#instance\n",
    "- http://boto3.readthedocs.io/en/latest/guide/migrationec2.html#launching-new-instances\n",
    "\n",
    "Task:\n",
    "- A larger task is to create an instance with Boto3, install an SSH client such as Paramaiko and run commands on the remote client.\n",
    "\n",
    "Helpful code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import paramiko\n",
    "\n",
    "\n",
    "ec2 = boto3.resource('ec2')\n",
    "instance = ec2.Instance('id')\n",
    "ec2.create_instances(ImageId='<ami-image-id>', MinCount=1, MaxCount=5)\n",
    "\n",
    "key = paramiko.RSAKey.from_private_key_file(path/to/mykey.pem)\n",
    "client = paramiko.SSHClient()\n",
    "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "# Connect/ssh to an instance\n",
    "try:\n",
    "    # Here 'ubuntu' is user name and 'instance_ip' is public IP of EC2\n",
    "    client.connect(hostname=instance_ip, username=\"ubuntu\", pkey=key)\n",
    "\n",
    "    # Execute a command(cmd) after connecting/ssh to an instance\n",
    "    stdin, stdout, stderr = client.exec_command(cmd)\n",
    "    print stdout.read()\n",
    "\n",
    "    # close the client connection once the job is done\n",
    "    client.close()\n",
    "    break\n",
    "\n",
    "except Exception, e:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When do we need containers? Using Docker.\n",
    "\n",
    "- Separating the environment from the operating system\n",
    "- Virtualization, VirtualBox, VMWare\n",
    "- Containers, Docker, Singularity, Vagrant\n",
    "- DockerHub\n",
    "- Backup the code, the data and the environment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ sudo docker run hello-world\n",
    "Unable to find image 'hello-world:latest' locally\n",
    "latest: Pulling from library/hello-world\n",
    "78445dd45222: Pull complete \n",
    "Digest: sha256:c5515758d4c5e1e838e9cd307f6c6a0d620b5e07e6f927b07d05f6d12a1ac8d7\n",
    "Status: Downloaded newer image for hello-world:latest\n",
    "\n",
    "Hello from Docker!\n",
    "This message shows that your installation appears to be working correctly.\n",
    "\n",
    "To generate this message, Docker took the following steps:\n",
    " 1. The Docker client contacted the Docker daemon.\n",
    " 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n",
    " 3. The Docker daemon created a new container from that image which runs the\n",
    "    executable that produces the output you are currently reading.\n",
    " 4. The Docker daemon streamed that output to the Docker client, which sent it\n",
    "    to your terminal.\n",
    "\n",
    "To try something more ambitious, you can run an Ubuntu container with:\n",
    " $ docker run -it ubuntu bash\n",
    "\n",
    "Share images, automate workflows, and more with a free Docker ID:\n",
    " https://cloud.docker.com/\n",
    "\n",
    "For more examples and ideas, visit:\n",
    " https://docs.docker.com/engine/userguide/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of a simple Dockerfile:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an official Python runtime as a base image\n",
    "FROM python:2.7-slim\n",
    "\n",
    "# Set the working directory to /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "ADD . /app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# Make port 80 available to the world outside this container\n",
    "EXPOSE 80\n",
    "\n",
    "# Define environment variable\n",
    "ENV NAME World\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the image:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ ls\n",
    "Dockerfile\t\tapp.py\t\t\trequirements.txt\n",
    "$ docker build -t friendlyhello .\n",
    "$ docker images\n",
    "\n",
    "REPOSITORY            TAG                 IMAGE ID\n",
    "friendlyhello         latest              326387cea398"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the image:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker run -d -p 4000:80 friendlyhello\n",
    "$ docker ps\n",
    "CONTAINER ID        IMAGE               COMMAND             CREATED\n",
    "1fa4ab2cf395        friendlyhello       \"python app.py\"     28 seconds ago\n",
    "$ docker stop 1fa4ab2cf395"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Publish on the docker hub and run on any computer on Earth:**\n",
    "\n",
    "```\n",
    "# signup at cloud.docker.com\n",
    "docker tag friendlyhello username/repository:tag\n",
    "docker push username/repository:tag\n",
    "```\n",
    "\n",
    "Now go to any computer on Earth having Docker installed and access to Internet and run:\n",
    "\n",
    "```\n",
    "docker run -p 4000:80 username/repository:tag\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spin up containers via Docker Machine**\n",
    "\n",
    "My preferred way is to use docker machine in order to manage cloud instances that are already set-up with Docker. Then you can pull your intended container from the Docker Hub and run it on the instance. An alternative is usign AWS services to create your instance, which has its own benefits (basically most benefits except for time). Another alternative is usign Docker Cloud or Kubernetes, which is the way to go for multiple instances.\n",
    "\n",
    "\n",
    "```\n",
    "# install docker machine\n",
    "$ base=https://github.com/docker/machine/releases/download/v0.14.0 && \\\n",
    "curl -L $base/docker-machine-$(uname -s)-$(uname -m) >/tmp/docker-machine && \\\n",
    "sudo install /tmp/docker-machine /usr/local/bin/docker-machine\n",
    "\n",
    "# setup a cloud instance\n",
    "$ export SECRET_KEY=\"...\"\n",
    "$ docker-machine create --driver amazonec2 --amazonec2-region eu-central-1 \\\n",
    "--amazonec2-access-key <your_key> \\\n",
    "--amazonec2-secret-key $SECRET_KEY \\\n",
    "aws-test\n",
    "\n",
    "# ssh and delete\n",
    "docker-machine ssh aws-test\n",
    "docker-machine rm aws-test\n",
    "\n",
    "# for other options: --amazonec2-instance-type \"t2.2xlarge\"\n",
    "docker-machine create --driver amazonec2\n",
    "```\n",
    "\n",
    "\n",
    "**Further read**\n",
    "\n",
    "- On Docker Machine:\n",
    "    - https://docs.docker.com/machine/examples/aws/\n",
    "    - https://docker-curriculum.com/\n",
    "    - https://sreeninet.wordpress.com/2016/09/03/docker-machine-for-aws/\n",
    "    - http://blog.wimwauters.com/docker-getting-started-with-docker-machine-aws/\n",
    "    - http://docker-k8s-lab.readthedocs.io/en/latest/docker/docker-machine-aws.html\n",
    "- Using AWS alternatives:\n",
    "    - https://aws.amazon.com/getting-started/tutorials/deploy-docker-containers/\n",
    "    - https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html#docker-basics-create-image\n",
    "    - https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\n",
    "- Or the more simplified use of Docker cloud, by linking to AWS. There are other, such as Kubernetes, but it is too complex (we only need one instance):\n",
    "    - https://docs.docker.com/docker-cloud/infrastructure/link-aws/\n",
    "    - https://docs.docker.com/docker-cloud/getting-started/your_first_node/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull the docker hub container on the EC2 instance, open shell and run test\n",
    "\n",
    "```\n",
    "# run inside the EC2 instance\n",
    "export DOCKER_ID_USER=\"<your_ID>\"\n",
    "docker login\n",
    "docker pull $DOCKER_ID_USER/awstest\n",
    "docker run -ti $DOCKER_ID_USER/awstest /bin/bash\n",
    "\n",
    "# now run your commands inside the container\n",
    "```\n",
    "\n",
    "This works only if you must run short tasks, because once you log out from the container, the container will end. What you need is to be able to run long jobs. So we must create a detached container, then attach to it during and after the execution of the program with a shell in order to check the logs and save data.\n",
    "\n",
    "```\n",
    "# run inside the EC2 instance\n",
    "export DOCKER_ID_USER=\"\"\n",
    "sudo docker login\n",
    "sudo docker pull $DOCKER_ID_USER/awscrass\n",
    "sudo docker run -w /home/ -tid $DOCKER_ID_USER/awscrass /bin/bash\n",
    "\n",
    "# exit container, start it\n",
    "sudo docker ps\n",
    "# run a command in detached mode\n",
    "#sudo docker exec -d containerid bash -c \"your command line\"\n",
    "\n",
    "#alternative is to log into the container and run the command there\n",
    "sudo docker exec -it containerid bash\n",
    "\n",
    "# start, attach\n",
    "docker start containerid\n",
    "docker attach containerid\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pricing**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- storage on S3 buckets:\n",
    "150 GB * 0.022$ /month = 3$\n",
    "- transfer:\n",
    "150 GB * 0.09$ /month = 13$\n",
    "- compute using m5.4xlarge (64GiB RAM) on demand instances:\n",
    "20 days * 0.9$/hour = 432$\n",
    "- compute using t2.2xlarge (32GiB RAM)\n",
    "20 days * 0.42$/hour = 201$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further read:\n",
    "- autoscaling? https://aws.amazon.com/autoscaling/\n",
    "- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html\n",
    "- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html\n",
    "- https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "\n",
    "Your task will be to create a Docker container, push it to Docker Hub, start an EC2 instance and remotely run your container, log out from the instance, then logging back and checking that the output is preserved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Types\n",
    "\n",
    "On AWS you can opt for different types of instances, and you can also upgrade or downgrade your instance to meet your need for resources. One can opt for example for instances that have a lot of RAM assigned when using a RAM intensive computation such as sequence alignment, or GPU instances when needing deep learning or other forms of GPU accelerated computing. More here: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html\n",
    "\n",
    "From the purchasing point of view there are however several major classes of instances, most notably:\n",
    "- On demand instances. Such instances are available when you request them and will be held up until you close them. - Spot instances. A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. \n",
    "- read about the other types here: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECS clusters and Docker Cloud\n",
    "\n",
    "\n",
    "You can run containerized clusters of EC2 instances using another AWS web service called ECS clusters. More information here: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_clusters.html\n",
    "\n",
    "Another popular option is to use Docker Cloud, allowing you to define and store images and build mechanisms of continuous integration then run tests or production clusters on AWS.\n",
    "\n",
    "Task:\n",
    "- Learn how to deply an Elastic HPC cluster: https://aws.amazon.com/getting-started/projects/deploy-elastic-hpc-cluster/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further read:\n",
    "- https://medium.com/big-data-on-amazon-elastic-mapreduce/run-a-spark-job-within-amazon-emr-in-15-minutes-68b02af1ae16\n",
    "- https://www.themarketingtechnologist.co/upload-your-local-spark-script-to-an-aws-emr-cluster-using-a-simply-python-script/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Spark on EC2 instances**\n",
    "\n",
    "\n",
    "Go to EMR and pick the cheapest option for your region, optimized for computing, mine is c5.xlarge.\n",
    "- https://aws.amazon.com/emr/pricing/?nc1=h_ls\n",
    "- https://aws.amazon.com/s3/pricing/\n",
    "\n",
    "The region settings for Stockholm can be found at:\n",
    "https://docs.aws.amazon.com/general/latest/gr/rande.html#apigateway_region\n",
    "I am using the Stockholm region (eu-north-1).\n",
    "\n",
    "Next you need access keys for IAM roles. Open https://console.aws.amazon.com/iam/ and folow the instructions:\n",
    "- https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ aws configure\n",
    "AWS Access Key ID [None]: [your_access_key]\n",
    "AWS Secret Access Key [None]:....\n",
    "Default region name [None]: eu-north-1\n",
    "Default output format [None]: json\n",
    "\n",
    "$ aws emr create-default-roles\n",
    "\n",
    "$ aws ec2 describe-subnets \\\n",
    ">      --filters \"Name=availabilityZone,Values=eu-north-1\"\n",
    "{\n",
    "    \"Subnets\": []\n",
    "}\n",
    "\n",
    "$ aws emr add-steps --cluster-id <your-cluster-job-id> --steps Name=Python Job,Jar=http://s3://elasticmapreduce/libs/script-runner/script-runner.jar,Args=[/home/hadoop/spark/bin/spark-submit,--deploy-mode,cluster,--master,yarn,s3://my_bucket/path/pythonjob.py,<comma separated list of arguments for your app>],ActionOnFailure=CONTINUE\n",
    "                    \n",
    "\n",
    "aws emr create-cluster \\\n",
    "--name \"sparkclust\" \\\n",
    "--release-label emr-5.23.0 \\\n",
    "--applications Name=Hadoop Name=Spark \\\n",
    "--ec2-attributes KeyName=spark_keypair \\\n",
    "--instance-groups \\\n",
    "Name=EmrMaster,InstanceGroupType=MASTER,InstanceCount=1,InstanceType=c5.xlarge \\\n",
    "Name=EmrCore,InstanceGroupType=CORE,InstanceCount=2,InstanceType=c5.xlarge \\\n",
    "--use-default-roles\n",
    "\n",
    "aws emr ssh --cluster-id [your id, ex: j-3H0XTRI8687P2] --key-pair-file /path/to/spark_keypair.pem\n",
    "\n",
    "\n",
    "$ aws emr list-clusters\n",
    "$ aws emr describe-cluster --cluster-id j-3H0XTRI8687P2\n",
    "# look for the \"MasterPublicDnsName\": \"ec2-13-48-55-199.eu-north-1.compute.amazonaws.com\"\n",
    "\n",
    "Before you can ssh to the master node you have to enable SSH to \"My IP\" (your computer's IP) on both the master and slave subnets.\n",
    "(visit Summary: Security groups for Master)\n",
    "\n",
    "# ssh example:\n",
    "ssh hadoop@ec2-13-48-55-199.eu-north-1.compute.amazonaws.com -i /home/sergiu/Downloads/spark_keypair.pem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used from https://github.com/apache/spark/blob/master/examples/src/main/python/pi.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "        Usage: pi [partitions]\n",
    "    \"\"\"\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPi\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
    "    n = 100000 * partitions\n",
    "\n",
    "    def f(_):\n",
    "        x = random() * 2 - 1\n",
    "        y = random() * 2 - 1\n",
    "        return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "    count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run cluster jobs interactively by SSH to the master node. But there are many other ways in which you can run your jobs on the spark cluster. Complete freedom!\n",
    "\n",
    "You can also add steps via the awscli and monitor them online:\n",
    "\n",
    "```\n",
    "$ aws emr add-steps \\\n",
    "--cluster-id j-3H0XTRI8687P2 \\\n",
    "--steps Type=CUSTOM_JAR,Name=\"Spark Program\",Jar=\"command-runner.jar\",ActionOnFailure=CONTINUE,Args=[\"spark-submit\",/home/hadoop/pitest.py]\n",
    "```\n",
    "\n",
    "**SUPER IMPORTANT STEP..**\n",
    "\n",
    "```\n",
    "aws emr terminate-clusters --cluster-ids j-3H0XTRI8687P2\n",
    "```\n",
    "\n",
    "OBS: Verify on https://eu-north-1.console.aws.amazon.com/elasticmapreduce/ that your cluster was terminated properly!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ ls /usr/lib/spark/python/lib/\n",
    "py4j-0.10.7-src.zip  PY4J_LICENSE.txt  py4j-src.zip  pyspark.zip\n",
    "[hadoop@ip-172-31-16-184 ~]$ export SPARK_HOME=/usr/lib/spark\n",
    "[hadoop@ip-172-31-16-184 ~]$ export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH\n",
    "[hadoop@ip-172-31-16-184 ~]$ export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/build:$PYTHONPATH\n",
    "[hadoop@ip-172-31-16-184 ~]$ source ~/.bashrc\n",
    "\n",
    "$ python pitest.py \n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "19/05/18 17:30:03 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
    "[Stage 0:>                                                          (0 + 0) / 2]19/05/18 17:30:19 WARN TaskSetManager: Stage 0 contains a task of very large size (371 KB). The maximum recommended task size is 100 KB.\n",
    "Pi is roughly 3.142540"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "- Log the result in an s3 bucket.\n",
    "- Try to load a data set and perform a basic ML task!\n",
    "- Configure JupyterHub via EMR, load giant pyspark steps from the comfort of your phone's web browser. Profit! ;)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "- Run Jupyter inside an EC2 instance. Examples:\n",
    "    - https://dataschool.com/data-modeling-101/running-jupyter-notebook-on-an-ec2-server/\n",
    "    - https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
