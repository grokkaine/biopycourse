{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba28ce22-df9a-45d4-b2e9-f756a5ff7aed",
   "metadata": {},
   "source": [
    "# Deep Learning, part3: Other important examples\n",
    "\n",
    "- Generative models: autoencoders and GANS\n",
    "- Working with tabular data, data integration\n",
    "- Recurrent NN and attention mechanisms\n",
    "- Reinforcement learning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1daec6d1-c747-4e55-ae54-5313b881a1d5",
   "metadata": {},
   "source": [
    "conda create -n biopy37 python=3.7\n",
    "conda install jupyterlab matplotlib tensorflow\n",
    "conda install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5556659-b6ee-4d9e-9fe7-f28697249db2",
   "metadata": {},
   "source": [
    "### Generative models: autoencoders, VAEs and GANS\n",
    "\n",
    "- **Generative models.** These are NNs models used for dimensionality reduction or dataset transformations.\n",
    "- A popular use for a NNs is to take its fitted weights and use them on other datasets. This is called **transfer learning**.\n",
    "- NNs need to verify information against a set of prior information in order to learn. In that sense, all NNs are supervised learning methods.\n",
    "- It is possible however to perform unsupervised learning with NNs, and the most popular method is auto-encoders. More precisely though, they are **self-supervised** because they generate their own labels from the training data.\n",
    "\n",
    "\n",
    "**Autoencoders**\n",
    "\n",
    "- A dimensionality reduction (or compression) NN algorithm in which the input of a model is the same as the output.\n",
    "- They compress the input into a lower-dimensional code and then reconstruct the output from this representation.\n",
    "- 3 components: encoder, code and decoder. The encoder compresses the input and produces the code, the decoder then reconstructs the input only using this code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a46fdd-f704-49f2-aee2-218ea0114511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../img/AE.png\" width=\"400\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"../img/AE.png\", width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a05108-d5a1-4b74-8186-62e08087b4d2",
   "metadata": {},
   "source": [
    "Autoencoders properties and usage:\n",
    "- Data-specific: Only able to meaningfully compress data similar to what they have been trained on. Autoencoders trained on handwritten digits won't compress landscape photos.\n",
    "- Lossy: The output of the autoencoder will not be exactly the same as the input, it will be a close but degraded representation.\n",
    "- Data denoising: By learning the relevant features they are able to denoise/normalize a dataset.\n",
    "- Clustering: Clustering algorithms struggle with large dimensional data, so AE are an important preprocessing step.\n",
    "- Generative models: Variational Autoencoders (VAE) learn the parameters of the probability distribution modeling the input data. By sampling points from this distribution we can also use the VAE as a generative model.\n",
    "\n",
    "\n",
    "**Tabular data**\n",
    "So far we have only used NNs on image and text (by converting them to numbers). Let's see an example of working directly with tabular data, which is more commonly used in 'omics research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff98b76-81ad-4f97-af4f-7ecc68686b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 136)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mRNA.H045</th>\n",
       "      <th>mRNA.H109</th>\n",
       "      <th>mRNA.H024</th>\n",
       "      <th>mRNA.H056</th>\n",
       "      <th>mRNA.H079</th>\n",
       "      <th>mRNA.H164</th>\n",
       "      <th>mRNA.H059</th>\n",
       "      <th>mRNA.H167</th>\n",
       "      <th>mRNA.H113</th>\n",
       "      <th>mRNA.H049</th>\n",
       "      <th>...</th>\n",
       "      <th>mRNA.H271</th>\n",
       "      <th>mRNA.H006</th>\n",
       "      <th>mRNA.H084</th>\n",
       "      <th>mRNA.H260</th>\n",
       "      <th>mRNA.H192</th>\n",
       "      <th>mRNA.H070</th>\n",
       "      <th>mRNA.H255</th>\n",
       "      <th>mRNA.H135</th>\n",
       "      <th>mRNA.H247</th>\n",
       "      <th>mRNA.H066</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000244734</th>\n",
       "      <td>4.558644</td>\n",
       "      <td>2.721512</td>\n",
       "      <td>9.938456</td>\n",
       "      <td>13.278004</td>\n",
       "      <td>6.086874</td>\n",
       "      <td>2.571839</td>\n",
       "      <td>4.938961</td>\n",
       "      <td>1.528848</td>\n",
       "      <td>2.286122</td>\n",
       "      <td>2.504699</td>\n",
       "      <td>...</td>\n",
       "      <td>4.199712</td>\n",
       "      <td>8.607476</td>\n",
       "      <td>10.682876</td>\n",
       "      <td>12.365431</td>\n",
       "      <td>6.731859</td>\n",
       "      <td>3.254823</td>\n",
       "      <td>3.269304</td>\n",
       "      <td>1.528848</td>\n",
       "      <td>8.826116</td>\n",
       "      <td>4.063590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000158528</th>\n",
       "      <td>11.741854</td>\n",
       "      <td>13.287432</td>\n",
       "      <td>2.341006</td>\n",
       "      <td>3.232874</td>\n",
       "      <td>11.940820</td>\n",
       "      <td>11.506818</td>\n",
       "      <td>5.483675</td>\n",
       "      <td>2.618869</td>\n",
       "      <td>2.812801</td>\n",
       "      <td>2.504699</td>\n",
       "      <td>...</td>\n",
       "      <td>3.743776</td>\n",
       "      <td>3.948041</td>\n",
       "      <td>2.651553</td>\n",
       "      <td>12.776441</td>\n",
       "      <td>3.071359</td>\n",
       "      <td>1.528848</td>\n",
       "      <td>12.427299</td>\n",
       "      <td>1.528848</td>\n",
       "      <td>3.121428</td>\n",
       "      <td>12.465548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000198478</th>\n",
       "      <td>8.921456</td>\n",
       "      <td>2.721512</td>\n",
       "      <td>12.381452</td>\n",
       "      <td>8.106266</td>\n",
       "      <td>4.889503</td>\n",
       "      <td>12.756213</td>\n",
       "      <td>3.593890</td>\n",
       "      <td>4.119490</td>\n",
       "      <td>5.220041</td>\n",
       "      <td>2.884897</td>\n",
       "      <td>...</td>\n",
       "      <td>2.226109</td>\n",
       "      <td>5.306285</td>\n",
       "      <td>9.321213</td>\n",
       "      <td>10.534619</td>\n",
       "      <td>6.091324</td>\n",
       "      <td>1.528848</td>\n",
       "      <td>2.907226</td>\n",
       "      <td>1.528848</td>\n",
       "      <td>8.087886</td>\n",
       "      <td>11.948637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000175445</th>\n",
       "      <td>12.686458</td>\n",
       "      <td>10.925985</td>\n",
       "      <td>1.528848</td>\n",
       "      <td>1.528848</td>\n",
       "      <td>13.340588</td>\n",
       "      <td>10.885547</td>\n",
       "      <td>11.194029</td>\n",
       "      <td>11.599981</td>\n",
       "      <td>2.286122</td>\n",
       "      <td>2.884897</td>\n",
       "      <td>...</td>\n",
       "      <td>2.226109</td>\n",
       "      <td>9.034459</td>\n",
       "      <td>9.397879</td>\n",
       "      <td>11.786520</td>\n",
       "      <td>1.528848</td>\n",
       "      <td>4.436292</td>\n",
       "      <td>11.425088</td>\n",
       "      <td>1.528848</td>\n",
       "      <td>5.680739</td>\n",
       "      <td>10.767604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000174469</th>\n",
       "      <td>2.644946</td>\n",
       "      <td>12.648355</td>\n",
       "      <td>1.528848</td>\n",
       "      <td>13.565210</td>\n",
       "      <td>5.476914</td>\n",
       "      <td>10.975187</td>\n",
       "      <td>7.944246</td>\n",
       "      <td>2.618869</td>\n",
       "      <td>2.286122</td>\n",
       "      <td>12.940957</td>\n",
       "      <td>...</td>\n",
       "      <td>13.723207</td>\n",
       "      <td>10.394117</td>\n",
       "      <td>12.091816</td>\n",
       "      <td>9.442299</td>\n",
       "      <td>4.948473</td>\n",
       "      <td>12.418931</td>\n",
       "      <td>1.528848</td>\n",
       "      <td>12.815852</td>\n",
       "      <td>9.970217</td>\n",
       "      <td>10.721614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mRNA.H045  mRNA.H109  mRNA.H024  mRNA.H056  mRNA.H079  \\\n",
       "ENSG00000244734   4.558644   2.721512   9.938456  13.278004   6.086874   \n",
       "ENSG00000158528  11.741854  13.287432   2.341006   3.232874  11.940820   \n",
       "ENSG00000198478   8.921456   2.721512  12.381452   8.106266   4.889503   \n",
       "ENSG00000175445  12.686458  10.925985   1.528848   1.528848  13.340588   \n",
       "ENSG00000174469   2.644946  12.648355   1.528848  13.565210   5.476914   \n",
       "\n",
       "                 mRNA.H164  mRNA.H059  mRNA.H167  mRNA.H113  mRNA.H049  ...  \\\n",
       "ENSG00000244734   2.571839   4.938961   1.528848   2.286122   2.504699  ...   \n",
       "ENSG00000158528  11.506818   5.483675   2.618869   2.812801   2.504699  ...   \n",
       "ENSG00000198478  12.756213   3.593890   4.119490   5.220041   2.884897  ...   \n",
       "ENSG00000175445  10.885547  11.194029  11.599981   2.286122   2.884897  ...   \n",
       "ENSG00000174469  10.975187   7.944246   2.618869   2.286122  12.940957  ...   \n",
       "\n",
       "                 mRNA.H271  mRNA.H006  mRNA.H084  mRNA.H260  mRNA.H192  \\\n",
       "ENSG00000244734   4.199712   8.607476  10.682876  12.365431   6.731859   \n",
       "ENSG00000158528   3.743776   3.948041   2.651553  12.776441   3.071359   \n",
       "ENSG00000198478   2.226109   5.306285   9.321213  10.534619   6.091324   \n",
       "ENSG00000175445   2.226109   9.034459   9.397879  11.786520   1.528848   \n",
       "ENSG00000174469  13.723207  10.394117  12.091816   9.442299   4.948473   \n",
       "\n",
       "                 mRNA.H070  mRNA.H255  mRNA.H135  mRNA.H247  mRNA.H066  \n",
       "ENSG00000244734   3.254823   3.269304   1.528848   8.826116   4.063590  \n",
       "ENSG00000158528   1.528848  12.427299   1.528848   3.121428  12.465548  \n",
       "ENSG00000198478   1.528848   2.907226   1.528848   8.087886  11.948637  \n",
       "ENSG00000175445   4.436292  11.425088   1.528848   5.680739  10.767604  \n",
       "ENSG00000174469  12.418931   1.528848  12.815852   9.970217  10.721614  \n",
       "\n",
       "[5 rows x 136 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "data_loc = r'D:\\windata\\work\\biopycourse\\data\\cll_data'\n",
    "df = pd.read_csv(pathlib.Path(data_loc) / \"cll_mrna.txt\", index_col=0, sep =\"\\t\")\n",
    "df = df.dropna(axis='columns')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1c19af99-1e05-4ffa-af70-0b7e923c874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3493dce9-166e-44bf-ba8f-0a5bcc00417b",
   "metadata": {},
   "source": [
    "**Goal**: reduce the dimensionality of this dataset from 5000 to 16, in order to efficiently cluster these samples.\n",
    "\n",
    "New learnings:\n",
    "- parametrization\n",
    "- batch normalization layers\n",
    "- naming layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a8314141-68b8-4fb9-8ec3-963c9d832bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Concatenate, Dense, Input, Lambda,Dropout\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "# elu, https://keras.io/activations/, maybe deals better with vanishing gradient\n",
    "#act = \"elu\"\n",
    "act = \"relu\"\n",
    "# the intermediate dense layers size\n",
    "ds = 128\n",
    "# latent space dimension size\n",
    "ls = 16\n",
    "# dropout rate [0 1]\n",
    "dropout = 0.2\n",
    "# ensure reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "297b58c8-bfd2-4652-8108-4092be66b177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 5000)]            0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 128)               640128    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "coded_layer (Dense)          (None, 16)                2064      \n",
      "=================================================================\n",
      "Total params: 642,704\n",
      "Trainable params: 642,448\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the encoder\n",
    "inputs_layer = Input(shape=(input_size,), name='input')\n",
    "x = Dense(ds, activation=act)(inputs_layer)\n",
    "x = BatchNormalization()(x)\n",
    "coded_layer = Dense(ls, name='coded_layer')(x)\n",
    "\n",
    "encoder = Model(inputs_layer, coded_layer, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bd3c7e7d-0c36-49a2-8d16-315395e21d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "latent_inputs (InputLayer)   [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 128)               2176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 5000)              645000    \n",
      "=================================================================\n",
      "Total params: 647,688\n",
      "Trainable params: 647,432\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the decoder\n",
    "decoder_inputs_layer = Input(shape=(ls,), name='latent_inputs')\n",
    "x = decoder_inputs_layer\n",
    "x = Dense(ds, activation=act)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout)(x)\n",
    "output_layer = Dense(input_size)(x)\n",
    "\n",
    "decoder = Model(decoder_inputs_layer, output_layer, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "31b37621-1419-44df-859f-68ac2fd7e5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 5000)]            0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         (None, 16)                642704    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 5000)              647688    \n",
      "=================================================================\n",
      "Total params: 1,290,392\n",
      "Trainable params: 1,289,880\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the autoencoder\n",
    "outputs = decoder(encoder(inputs_layer))\n",
    "autoencoder = Model(inputs_layer, outputs, name='autoencoder')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8908b-ef49-4c5c-9501-47f63a85e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and run\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=False)\n",
    "autoencoder.compile(loss='mse', optimizer=adam, metrics=['accuracy'])\n",
    "#history = autoencoder.fit(X_train, X_train, epochs=5, batch_size=32, shuffle=True, validation_data=(X_test, X_test))\n",
    "history = autoencoder.fit(X_train, X_train, epochs=200, batch_size=64, shuffle=True)\n",
    "autoencoder.save('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ab76df07-2985-45ca-9542-acd1611cc228",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_X_train = encoder.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0d2aede0-1875-4e43-9d47-3bdc23f4946a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 16)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79c142-a70e-4e86-ac8e-bcb32d5f3807",
   "metadata": {},
   "source": [
    "Task:\n",
    "- Run KMeans both before and after dimensionality reduction, and plot their silhouette scores. Is there an improvement?\n",
    "- (advanced) Expand the AE above into a VAE, and repeat clustering assesment.\n",
    "- Using the above hyperparameters try to improve the model fit and re-asses clustering performance.\n",
    "- (really advanced) Search for a VAE-GANS implementation and re run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd2306-a3dd-400a-8fe7-be120110cf2b",
   "metadata": {},
   "source": [
    "## What are GANS?\n",
    "\n",
    "-  “the most interesting idea in the last 10 years in Machine Learning” (Ian LeCun)\n",
    "- Generator model: the goal of the generator is to fool the discriminator, so the generative neural network is trained to maximise the final classification error (between true and generated data)\n",
    "- Discriminator model: the goal of the discriminator is to detect fake generated data, so the discriminative neural network is trained to minimise the final classification error\n",
    "\n",
    "Example for MNIST:\n",
    "- https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccf422-ee19-447c-a7f4-da21fa4fd2ce",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "\n",
    "These networks process (loop) the information several times through every node. Such networks are mainly applied with the purpose of classifying sequential input and rely on backpropagation of error to do so. When the information passes a single time, the network is called feed-forward. Recurrent networks, on the other hand, take as their input not just the current input example they see, but also what they have perceived previously in time. Thus a RNN uses the concept of time and memory.\n",
    "\n",
    "One could, for example, define the activation function on a hidden state in this manner, by a method called backpropagation through time:\n",
    "output_t = relu(dot(W, input) + dot(U, output.t-1))\n",
    "\n",
    "A traditional deep neural network uses different parameters at each layer, while a RNN shares the same parameters across all steps. The output of each time step doesn't need to be kept (not necessarily). We not care for example while doing sentiment analysis about the output after every word.\n",
    "\n",
    "Features:\n",
    "- they can be bi-directional\n",
    "- they can be deep (multiple layers per time step)\n",
    "- RNNs can be combined with CNNs to solve complex problems, from speech or image recognition to machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf624939-5fd9-4b06-a515-f753df6b1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60828f92-18d6-4138-b805-76616b43e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install numpy\">=1.19.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1401e13e-b1e7-47d7-ba72-a220fc18693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.20.1\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c9254-9da7-46b6-b78b-9a153bbb0208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
