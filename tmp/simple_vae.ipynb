{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple variational autoencoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.layers import BatchNormalization as BN, Concatenate, Dense, Input, Lambda,Dropout\n",
    "from keras.models import Model\n",
    "from keras.losses import mean_squared_error, binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9248, 135)\n"
     ]
    }
   ],
   "source": [
    "data_loc = \"/media/sergiu/workpc/tmp/iomics/cll_data/\"\n",
    "import pandas as pd\n",
    "df_meth = pd.read_csv(data_loc + \"CLL_data_Methylation.csv\", index_col=0)\n",
    "df_mrna = pd.read_csv(data_loc + \"CLL_data_mRNA.csv\", index_col=0)\n",
    "\n",
    "# drop nans by column\n",
    "df_mrna = df_mrna.dropna(axis='columns')\n",
    "df_meth = df_meth.dropna(axis='columns')\n",
    "\n",
    "X = pd.concat([df_mrna, df_meth])\n",
    "X = X.dropna(axis='columns')\n",
    "print(X.shape)\n",
    "X_train, X_test = train_test_split(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "concat_input (InputLayer)       [(None, 135)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          17408       concat_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 16)           2064        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "z_log_sigma (Dense)             (None, 16)           2064        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 16)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_sigma[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 22,048\n",
      "Trainable params: 21,792\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               2176      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 135)               17415     \n",
      "=================================================================\n",
      "Total params: 20,103\n",
      "Trainable params: 19,847\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Model: \"vae_mlp\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "concat_input (InputLayer)       [(None, 135)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            [(None, 16), (None,  22048       concat_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, 135)          20103       encoder[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          17408       concat_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_sigma (Dense)             (None, 16)           2064        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 16)           2064        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_2 (TensorFlow [(None, 16)]         0           z_log_sigma[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Square_1 (TensorFlo [(None, 16)]         0           z_mean[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_2 (TensorFlowOp [(None, 16)]         0           tf_op_layer_AddV2_2[0][0]        \n",
      "                                                                 tf_op_layer_Square_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_1 (TensorFlowOp [(None, 16)]         0           z_log_sigma[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_3 (TensorFlowOp [(None, 16)]         0           tf_op_layer_Sub_2[0][0]          \n",
      "                                                                 tf_op_layer_Exp_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_1 (TensorFlowOp [(None,)]            0           tf_op_layer_Sub_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_SquaredDifference_1 [(None, 135)]        0           decoder[0][0]                    \n",
      "                                                                 concat_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_2 (TensorFlowOp [(None,)]            0           tf_op_layer_Sum_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_2 (TensorFlowO [(None,)]            0           tf_op_layer_SquaredDifference_1[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_3 (TensorFlowOp [(None,)]            0           tf_op_layer_Mul_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_3 (TensorFlow [(None,)]            0           tf_op_layer_Mean_2[0][0]         \n",
      "                                                                 tf_op_layer_Mul_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_3 (TensorFlowO [()]                 0           tf_op_layer_AddV2_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_loss_1 (AddLoss)            ()                   0           tf_op_layer_Mean_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 42,151\n",
      "Trainable params: 41,639\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# s1_train.shape[1]+s2_train.shape[1]\n",
    "#input_size = s1_train.shape[1]+s2_train.shape[1]\n",
    "input_size = 135\n",
    "# relu?, https://keras.io/activations/\n",
    "act = \"elu\"\n",
    "# the intermediate dense layers size: 128 256 512\n",
    "ds = 128\n",
    "# latent space dimension size 16 32 64\n",
    "ls = 16\n",
    "# dropout rate [0 1]\n",
    "dropout = 0.2\n",
    "# KL adjustement parameter [0 1]?, 1 10 15 25 50 100 ?\n",
    "beta = 1\n",
    "\n",
    "# build the model\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build the encoder network\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(input_size,), name='concat_input')\n",
    "#inputs = [concat_inputs]\n",
    "\n",
    "# Encoding layer\n",
    "x = Dense(ds, activation=act)(inputs)\n",
    "x = BN()(x)      \n",
    "\n",
    "# Embedding layer\n",
    "z_mean = Dense(ls, name='z_mean')(x)\n",
    "z_log_sigma = Dense(ls, name='z_log_sigma', kernel_initializer='zeros')(x)\n",
    "z = Lambda(sampling, output_shape=(ls,), name='z')([z_mean, z_log_sigma])\n",
    "\n",
    "encoder = Model(inputs, [z_mean, z_log_sigma, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "# Build the decoder network\n",
    "\n",
    "# Dense out\n",
    "latent_inputs = Input(shape=(ls,), name='z_sampling')\n",
    "x = latent_inputs\n",
    "x = Dense(ds, activation=act)(x)\n",
    "x = BN()(x)\n",
    "\n",
    "x = Dropout(dropout)(x)\n",
    "\n",
    "# Out\n",
    "concat_out = Dense(input_size)(x)\n",
    "\n",
    "decoder = Model(latent_inputs, concat_out, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "# Define the loss\n",
    "distance = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)\n",
    "distance = K.sum(distance, axis=-1)\n",
    "distance *= -0.5\n",
    "reconstruction_loss = mean_squared_error(inputs, outputs)\n",
    "vae_loss = K.mean(reconstruction_loss + beta * distance)\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001, amsgrad=False)\n",
    "vae.compile(optimizer=adam)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/450\n",
      "28/28 [==============================] - 0s 9ms/step - loss: 33.8306 - val_loss: 32.4086\n",
      "Epoch 2/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 22.0966 - val_loss: 16.5133\n",
      "Epoch 3/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 18.2590 - val_loss: 15.2582\n",
      "Epoch 4/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 16.6531 - val_loss: 14.2816\n",
      "Epoch 5/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 15.1592 - val_loss: 13.4528\n",
      "Epoch 6/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 13.3647 - val_loss: 11.7242\n",
      "Epoch 7/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 11.5131 - val_loss: 10.7749\n",
      "Epoch 8/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 10.1072 - val_loss: 9.2994\n",
      "Epoch 9/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 9.4524 - val_loss: 8.9485\n",
      "Epoch 10/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 8.8227 - val_loss: 8.5754\n",
      "Epoch 11/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 8.5071 - val_loss: 8.2975\n",
      "Epoch 12/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 8.4225 - val_loss: 8.0816\n",
      "Epoch 13/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 8.4315 - val_loss: 7.9163\n",
      "Epoch 14/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 8.2794 - val_loss: 7.8419\n",
      "Epoch 15/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 8.2729 - val_loss: 7.8045\n",
      "Epoch 16/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 8.2157 - val_loss: 7.6428\n",
      "Epoch 17/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.9538 - val_loss: 7.3862\n",
      "Epoch 18/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.8001 - val_loss: 7.2325\n",
      "Epoch 19/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.6971 - val_loss: 7.0906\n",
      "Epoch 20/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.5716 - val_loss: 7.1392\n",
      "Epoch 21/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.4103 - val_loss: 6.9751\n",
      "Epoch 22/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.4448 - val_loss: 6.9908\n",
      "Epoch 23/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.3891 - val_loss: 6.9804\n",
      "Epoch 24/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.3402 - val_loss: 7.0512\n",
      "Epoch 25/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.3583 - val_loss: 7.1113\n",
      "Epoch 26/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.3414 - val_loss: 7.0339\n",
      "Epoch 27/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.3519 - val_loss: 6.9026\n",
      "Epoch 28/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2825 - val_loss: 6.8994\n",
      "Epoch 29/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2846 - val_loss: 6.8050\n",
      "Epoch 30/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2610 - val_loss: 6.8738\n",
      "Epoch 31/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2597 - val_loss: 6.8419\n",
      "Epoch 32/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2613 - val_loss: 6.8350\n",
      "Epoch 33/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2525 - val_loss: 6.9177\n",
      "Epoch 34/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2305 - val_loss: 6.8930\n",
      "Epoch 35/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2971 - val_loss: 6.8245\n",
      "Epoch 36/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.3052 - val_loss: 6.8950\n",
      "Epoch 37/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1769 - val_loss: 6.8134\n",
      "Epoch 38/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2452 - val_loss: 6.8489\n",
      "Epoch 39/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2194 - val_loss: 6.7284\n",
      "Epoch 40/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.3104 - val_loss: 6.8782\n",
      "Epoch 41/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2338 - val_loss: 6.9793\n",
      "Epoch 42/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1942 - val_loss: 6.9053\n",
      "Epoch 43/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1894 - val_loss: 6.7677\n",
      "Epoch 44/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.3892 - val_loss: 6.8148\n",
      "Epoch 45/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1739 - val_loss: 6.8635\n",
      "Epoch 46/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1908 - val_loss: 6.7720\n",
      "Epoch 47/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1154 - val_loss: 6.8320\n",
      "Epoch 48/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2020 - val_loss: 6.7505\n",
      "Epoch 49/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2464 - val_loss: 6.7949\n",
      "Epoch 50/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1670 - val_loss: 6.7732\n",
      "Epoch 51/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0884 - val_loss: 6.8370\n",
      "Epoch 52/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1385 - val_loss: 6.8246\n",
      "Epoch 53/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0891 - val_loss: 6.8048\n",
      "Epoch 54/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1559 - val_loss: 6.8241\n",
      "Epoch 55/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1373 - val_loss: 6.7731\n",
      "Epoch 56/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2053 - val_loss: 6.7538\n",
      "Epoch 57/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2272 - val_loss: 6.7888\n",
      "Epoch 58/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1676 - val_loss: 6.7829\n",
      "Epoch 59/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1353 - val_loss: 6.7885\n",
      "Epoch 60/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1059 - val_loss: 6.7648\n",
      "Epoch 61/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1999 - val_loss: 6.8695\n",
      "Epoch 62/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0952 - val_loss: 6.8289\n",
      "Epoch 63/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0829 - val_loss: 6.7158\n",
      "Epoch 64/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0839 - val_loss: 6.7350\n",
      "Epoch 65/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1445 - val_loss: 6.8141\n",
      "Epoch 66/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0538 - val_loss: 6.7541\n",
      "Epoch 67/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0830 - val_loss: 6.7530\n",
      "Epoch 68/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0770 - val_loss: 6.7534\n",
      "Epoch 69/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0759 - val_loss: 6.7649\n",
      "Epoch 70/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0943 - val_loss: 6.7336\n",
      "Epoch 71/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0676 - val_loss: 6.7818\n",
      "Epoch 72/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1666 - val_loss: 6.7776\n",
      "Epoch 73/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1410 - val_loss: 6.7646\n",
      "Epoch 74/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0603 - val_loss: 6.6936\n",
      "Epoch 75/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1007 - val_loss: 6.7818\n",
      "Epoch 76/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0826 - val_loss: 6.7892\n",
      "Epoch 77/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0616 - val_loss: 6.7264\n",
      "Epoch 78/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0908 - val_loss: 6.8245\n",
      "Epoch 79/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0395 - val_loss: 6.7601\n",
      "Epoch 80/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1647 - val_loss: 6.7666\n",
      "Epoch 81/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0675 - val_loss: 6.7466\n",
      "Epoch 82/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0889 - val_loss: 6.7147\n",
      "Epoch 83/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0193 - val_loss: 6.7279\n",
      "Epoch 84/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0506 - val_loss: 6.7225\n",
      "Epoch 85/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0214 - val_loss: 6.7804\n",
      "Epoch 86/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0003 - val_loss: 6.7367\n",
      "Epoch 87/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1398 - val_loss: 6.7267\n",
      "Epoch 88/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1449 - val_loss: 6.8087\n",
      "Epoch 89/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0481 - val_loss: 6.7198\n",
      "Epoch 90/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0634 - val_loss: 6.7174\n",
      "Epoch 91/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0608 - val_loss: 6.7721\n",
      "Epoch 92/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9986 - val_loss: 6.7124\n",
      "Epoch 93/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9688 - val_loss: 6.8218\n",
      "Epoch 94/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0593 - val_loss: 6.7857\n",
      "Epoch 95/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0216 - val_loss: 6.6980\n",
      "Epoch 96/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9935 - val_loss: 6.7930\n",
      "Epoch 97/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0696 - val_loss: 6.6676\n",
      "Epoch 98/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9808 - val_loss: 6.7235\n",
      "Epoch 99/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0467 - val_loss: 6.7459\n",
      "Epoch 100/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0662 - val_loss: 6.8271\n",
      "Epoch 101/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0411 - val_loss: 6.6629\n",
      "Epoch 102/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0521 - val_loss: 6.7072\n",
      "Epoch 103/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2719 - val_loss: 6.6993\n",
      "Epoch 104/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0215 - val_loss: 6.7087\n",
      "Epoch 105/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0583 - val_loss: 6.7261\n",
      "Epoch 106/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9956 - val_loss: 6.7190\n",
      "Epoch 107/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0276 - val_loss: 6.7420\n",
      "Epoch 108/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0032 - val_loss: 6.7587\n",
      "Epoch 109/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.2035 - val_loss: 6.7112\n",
      "Epoch 110/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0674 - val_loss: 6.7512\n",
      "Epoch 111/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0289 - val_loss: 6.7272\n",
      "Epoch 112/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0342 - val_loss: 6.7291\n",
      "Epoch 113/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9683 - val_loss: 6.6980\n",
      "Epoch 114/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0150 - val_loss: 6.8004\n",
      "Epoch 115/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9709 - val_loss: 6.7425\n",
      "Epoch 116/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0338 - val_loss: 6.7556\n",
      "Epoch 117/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9574 - val_loss: 6.7245\n",
      "Epoch 118/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1133 - val_loss: 6.7703\n",
      "Epoch 119/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1192 - val_loss: 6.6921\n",
      "Epoch 120/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9977 - val_loss: 6.6985\n",
      "Epoch 121/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0049 - val_loss: 6.6968\n",
      "Epoch 122/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9612 - val_loss: 6.7654\n",
      "Epoch 123/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9589 - val_loss: 6.7495\n",
      "Epoch 124/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0186 - val_loss: 6.7916\n",
      "Epoch 125/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9859 - val_loss: 6.7166\n",
      "Epoch 126/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0146 - val_loss: 6.7260\n",
      "Epoch 127/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9549 - val_loss: 6.7297\n",
      "Epoch 128/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9264 - val_loss: 6.7190\n",
      "Epoch 129/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0438 - val_loss: 6.7296\n",
      "Epoch 130/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0952 - val_loss: 6.7252\n",
      "Epoch 131/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9555 - val_loss: 6.7439\n",
      "Epoch 132/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9730 - val_loss: 6.6969\n",
      "Epoch 133/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0959 - val_loss: 6.7195\n",
      "Epoch 134/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0217 - val_loss: 6.7445\n",
      "Epoch 135/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9545 - val_loss: 6.7211\n",
      "Epoch 136/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9903 - val_loss: 6.7136\n",
      "Epoch 137/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0202 - val_loss: 6.6681\n",
      "Epoch 138/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9527 - val_loss: 6.7259\n",
      "Epoch 139/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9360 - val_loss: 6.7599\n",
      "Epoch 140/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0567 - val_loss: 6.6949\n",
      "Epoch 141/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0022 - val_loss: 6.8212\n",
      "Epoch 142/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0858 - val_loss: 6.7391\n",
      "Epoch 143/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9157 - val_loss: 6.7126\n",
      "Epoch 144/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9741 - val_loss: 6.6853\n",
      "Epoch 145/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9537 - val_loss: 6.7118\n",
      "Epoch 146/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1463 - val_loss: 6.7384\n",
      "Epoch 147/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9818 - val_loss: 6.7567\n",
      "Epoch 148/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0340 - val_loss: 6.7344\n",
      "Epoch 149/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9933 - val_loss: 6.6557\n",
      "Epoch 150/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9098 - val_loss: 6.6378\n",
      "Epoch 151/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0030 - val_loss: 6.6470\n",
      "Epoch 152/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9344 - val_loss: 6.7757\n",
      "Epoch 153/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0378 - val_loss: 6.7448\n",
      "Epoch 154/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9827 - val_loss: 6.6764\n",
      "Epoch 155/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9752 - val_loss: 6.6862\n",
      "Epoch 156/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9276 - val_loss: 6.7061\n",
      "Epoch 157/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9517 - val_loss: 6.6796\n",
      "Epoch 158/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9936 - val_loss: 6.7300\n",
      "Epoch 159/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9705 - val_loss: 6.6251\n",
      "Epoch 160/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9404 - val_loss: 6.7079\n",
      "Epoch 161/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0035 - val_loss: 6.7024\n",
      "Epoch 162/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9577 - val_loss: 6.7078\n",
      "Epoch 163/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9497 - val_loss: 6.7027\n",
      "Epoch 164/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9709 - val_loss: 6.7055\n",
      "Epoch 165/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0347 - val_loss: 6.6957\n",
      "Epoch 166/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9550 - val_loss: 6.6930\n",
      "Epoch 167/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9137 - val_loss: 6.7661\n",
      "Epoch 168/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1309 - val_loss: 6.7092\n",
      "Epoch 169/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9234 - val_loss: 6.6811\n",
      "Epoch 170/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9297 - val_loss: 6.7883\n",
      "Epoch 171/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9125 - val_loss: 6.6894\n",
      "Epoch 172/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9967 - val_loss: 6.7043\n",
      "Epoch 173/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9634 - val_loss: 6.6660\n",
      "Epoch 174/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0702 - val_loss: 6.7151\n",
      "Epoch 175/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9638 - val_loss: 6.6555\n",
      "Epoch 176/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9613 - val_loss: 6.7246\n",
      "Epoch 177/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9371 - val_loss: 6.7135\n",
      "Epoch 178/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9001 - val_loss: 6.6833\n",
      "Epoch 179/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9468 - val_loss: 6.6904\n",
      "Epoch 180/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9346 - val_loss: 6.7293\n",
      "Epoch 181/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8770 - val_loss: 6.9452\n",
      "Epoch 182/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9129 - val_loss: 6.7432\n",
      "Epoch 183/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8954 - val_loss: 6.6799\n",
      "Epoch 184/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8892 - val_loss: 6.7121\n",
      "Epoch 185/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9498 - val_loss: 6.6906\n",
      "Epoch 186/450\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 6.9496 - val_loss: 6.6772\n",
      "Epoch 187/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9100 - val_loss: 6.6157\n",
      "Epoch 188/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0022 - val_loss: 6.6343\n",
      "Epoch 189/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9717 - val_loss: 6.6588\n",
      "Epoch 190/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0699 - val_loss: 6.7033\n",
      "Epoch 191/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9351 - val_loss: 6.7308\n",
      "Epoch 192/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9237 - val_loss: 6.7147\n",
      "Epoch 193/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8965 - val_loss: 6.7058\n",
      "Epoch 194/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8893 - val_loss: 6.6732\n",
      "Epoch 195/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8802 - val_loss: 6.6975\n",
      "Epoch 196/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9133 - val_loss: 6.7155\n",
      "Epoch 197/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9549 - val_loss: 6.7246\n",
      "Epoch 198/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9290 - val_loss: 6.7333\n",
      "Epoch 199/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0043 - val_loss: 6.6086\n",
      "Epoch 200/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9216 - val_loss: 6.7693\n",
      "Epoch 201/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0357 - val_loss: 6.6777\n",
      "Epoch 202/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9207 - val_loss: 6.6808\n",
      "Epoch 203/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9662 - val_loss: 6.6523\n",
      "Epoch 204/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9158 - val_loss: 6.7043\n",
      "Epoch 205/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9234 - val_loss: 6.6455\n",
      "Epoch 206/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9189 - val_loss: 6.6582\n",
      "Epoch 207/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0346 - val_loss: 6.6756\n",
      "Epoch 208/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9695 - val_loss: 6.6330\n",
      "Epoch 209/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9144 - val_loss: 6.6751\n",
      "Epoch 210/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8757 - val_loss: 6.7078\n",
      "Epoch 211/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9506 - val_loss: 6.7526\n",
      "Epoch 212/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8951 - val_loss: 6.6363\n",
      "Epoch 213/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8937 - val_loss: 6.6622\n",
      "Epoch 214/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9616 - val_loss: 6.6461\n",
      "Epoch 215/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9124 - val_loss: 6.6091\n",
      "Epoch 216/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0867 - val_loss: 6.6950\n",
      "Epoch 217/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9278 - val_loss: 6.6768\n",
      "Epoch 218/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8812 - val_loss: 6.6714\n",
      "Epoch 219/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8875 - val_loss: 6.7081\n",
      "Epoch 220/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9183 - val_loss: 6.7010\n",
      "Epoch 221/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9116 - val_loss: 6.6469\n",
      "Epoch 222/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9253 - val_loss: 6.6881\n",
      "Epoch 223/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9415 - val_loss: 6.7072\n",
      "Epoch 224/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0365 - val_loss: 6.7385\n",
      "Epoch 225/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9763 - val_loss: 6.7163\n",
      "Epoch 226/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9597 - val_loss: 6.6258\n",
      "Epoch 227/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8742 - val_loss: 6.6574\n",
      "Epoch 228/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9694 - val_loss: 6.6930\n",
      "Epoch 229/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8800 - val_loss: 6.6996\n",
      "Epoch 230/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9752 - val_loss: 6.6982\n",
      "Epoch 231/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9506 - val_loss: 6.6813\n",
      "Epoch 232/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0228 - val_loss: 6.6263\n",
      "Epoch 233/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8903 - val_loss: 6.6832\n",
      "Epoch 234/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8914 - val_loss: 6.6529\n",
      "Epoch 235/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9596 - val_loss: 6.6693\n",
      "Epoch 236/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9065 - val_loss: 6.7116\n",
      "Epoch 237/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9375 - val_loss: 6.6850\n",
      "Epoch 238/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8849 - val_loss: 6.7008\n",
      "Epoch 239/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0881 - val_loss: 6.6444\n",
      "Epoch 240/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9158 - val_loss: 6.6278\n",
      "Epoch 241/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9197 - val_loss: 6.6902\n",
      "Epoch 242/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8934 - val_loss: 6.6992\n",
      "Epoch 243/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9277 - val_loss: 6.6597\n",
      "Epoch 244/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9200 - val_loss: 6.6911\n",
      "Epoch 245/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9187 - val_loss: 6.7602\n",
      "Epoch 246/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8944 - val_loss: 6.6201\n",
      "Epoch 247/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9181 - val_loss: 6.6916\n",
      "Epoch 248/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8707 - val_loss: 6.6153\n",
      "Epoch 249/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9292 - val_loss: 6.6915\n",
      "Epoch 250/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9707 - val_loss: 6.6624\n",
      "Epoch 251/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9998 - val_loss: 6.6351\n",
      "Epoch 252/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0518 - val_loss: 6.6767\n",
      "Epoch 253/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9043 - val_loss: 6.6448\n",
      "Epoch 254/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9113 - val_loss: 6.7489\n",
      "Epoch 255/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8956 - val_loss: 6.7299\n",
      "Epoch 256/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8477 - val_loss: 6.6335\n",
      "Epoch 257/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8967 - val_loss: 6.6739\n",
      "Epoch 258/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0263 - val_loss: 6.7160\n",
      "Epoch 259/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9165 - val_loss: 6.6453\n",
      "Epoch 260/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9304 - val_loss: 6.6824\n",
      "Epoch 261/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0897 - val_loss: 6.6349\n",
      "Epoch 262/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8610 - val_loss: 6.6232\n",
      "Epoch 263/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9619 - val_loss: 6.6414\n",
      "Epoch 264/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9082 - val_loss: 6.7792\n",
      "Epoch 265/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9121 - val_loss: 6.6608\n",
      "Epoch 266/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9577 - val_loss: 6.6712\n",
      "Epoch 267/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9373 - val_loss: 6.6792\n",
      "Epoch 268/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9272 - val_loss: 6.6252\n",
      "Epoch 269/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8894 - val_loss: 6.6445\n",
      "Epoch 270/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9042 - val_loss: 6.6748\n",
      "Epoch 271/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9087 - val_loss: 6.6379\n",
      "Epoch 272/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8396 - val_loss: 6.6597\n",
      "Epoch 273/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9446 - val_loss: 6.6624\n",
      "Epoch 274/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9155 - val_loss: 6.6333\n",
      "Epoch 275/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8642 - val_loss: 6.7541\n",
      "Epoch 276/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9282 - val_loss: 6.6454\n",
      "Epoch 277/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9008 - val_loss: 6.6134\n",
      "Epoch 278/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8871 - val_loss: 6.6713\n",
      "Epoch 279/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8492 - val_loss: 6.6718\n",
      "Epoch 280/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8898 - val_loss: 6.6809\n",
      "Epoch 281/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9453 - val_loss: 6.6244\n",
      "Epoch 282/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9023 - val_loss: 6.6192\n",
      "Epoch 283/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8955 - val_loss: 6.6466\n",
      "Epoch 284/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8652 - val_loss: 6.6159\n",
      "Epoch 285/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9304 - val_loss: 6.6562\n",
      "Epoch 286/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9556 - val_loss: 6.7024\n",
      "Epoch 287/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8481 - val_loss: 6.6148\n",
      "Epoch 288/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9921 - val_loss: 6.6287\n",
      "Epoch 289/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8807 - val_loss: 6.6119\n",
      "Epoch 290/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9059 - val_loss: 6.6572\n",
      "Epoch 291/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9177 - val_loss: 6.6604\n",
      "Epoch 292/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9987 - val_loss: 6.6218\n",
      "Epoch 293/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8887 - val_loss: 6.6918\n",
      "Epoch 294/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8725 - val_loss: 6.6654\n",
      "Epoch 295/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9096 - val_loss: 6.6384\n",
      "Epoch 296/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8779 - val_loss: 6.6886\n",
      "Epoch 297/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8535 - val_loss: 6.6755\n",
      "Epoch 298/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9293 - val_loss: 6.6480\n",
      "Epoch 299/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9562 - val_loss: 6.6686\n",
      "Epoch 300/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8731 - val_loss: 6.6222\n",
      "Epoch 301/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8483 - val_loss: 6.6501\n",
      "Epoch 302/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9821 - val_loss: 6.6184\n",
      "Epoch 303/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8799 - val_loss: 6.6256\n",
      "Epoch 304/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9209 - val_loss: 6.7068\n",
      "Epoch 305/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8921 - val_loss: 6.6386\n",
      "Epoch 306/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9542 - val_loss: 6.6952\n",
      "Epoch 307/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9318 - val_loss: 6.7045\n",
      "Epoch 308/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9110 - val_loss: 6.5821\n",
      "Epoch 309/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8993 - val_loss: 6.6800\n",
      "Epoch 310/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9142 - val_loss: 6.7011\n",
      "Epoch 311/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9021 - val_loss: 6.6230\n",
      "Epoch 312/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.1280 - val_loss: 6.6363\n",
      "Epoch 313/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8559 - val_loss: 6.6314\n",
      "Epoch 314/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8715 - val_loss: 6.6791\n",
      "Epoch 315/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8554 - val_loss: 6.6329\n",
      "Epoch 316/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9640 - val_loss: 6.6332\n",
      "Epoch 317/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9392 - val_loss: 6.6893\n",
      "Epoch 318/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8782 - val_loss: 6.6493\n",
      "Epoch 319/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9053 - val_loss: 6.6625\n",
      "Epoch 320/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8781 - val_loss: 6.6814\n",
      "Epoch 321/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8906 - val_loss: 6.6320\n",
      "Epoch 322/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9247 - val_loss: 6.7194\n",
      "Epoch 323/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9232 - val_loss: 6.6471\n",
      "Epoch 324/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9066 - val_loss: 6.5934\n",
      "Epoch 325/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8941 - val_loss: 6.6954\n",
      "Epoch 326/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8906 - val_loss: 6.6120\n",
      "Epoch 327/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9114 - val_loss: 6.6997\n",
      "Epoch 328/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8930 - val_loss: 6.6585\n",
      "Epoch 329/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9012 - val_loss: 6.6630\n",
      "Epoch 330/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8835 - val_loss: 6.6453\n",
      "Epoch 331/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8490 - val_loss: 6.6934\n",
      "Epoch 332/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9352 - val_loss: 6.6484\n",
      "Epoch 333/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9812 - val_loss: 6.6595\n",
      "Epoch 334/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9498 - val_loss: 6.6064\n",
      "Epoch 335/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9740 - val_loss: 6.6181\n",
      "Epoch 336/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9961 - val_loss: 6.5982\n",
      "Epoch 337/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9403 - val_loss: 6.6006\n",
      "Epoch 338/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0479 - val_loss: 6.6066\n",
      "Epoch 339/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8579 - val_loss: 6.6662\n",
      "Epoch 340/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8942 - val_loss: 6.6893\n",
      "Epoch 341/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9043 - val_loss: 6.6940\n",
      "Epoch 342/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8666 - val_loss: 6.6101\n",
      "Epoch 343/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8571 - val_loss: 6.6600\n",
      "Epoch 344/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9239 - val_loss: 6.6597\n",
      "Epoch 345/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8721 - val_loss: 6.6301\n",
      "Epoch 346/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9050 - val_loss: 6.6495\n",
      "Epoch 347/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8179 - val_loss: 6.7331\n",
      "Epoch 348/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8824 - val_loss: 6.6852\n",
      "Epoch 349/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8865 - val_loss: 6.6288\n",
      "Epoch 350/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8572 - val_loss: 6.7307\n",
      "Epoch 351/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8483 - val_loss: 6.7040\n",
      "Epoch 352/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8578 - val_loss: 6.6408\n",
      "Epoch 353/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9059 - val_loss: 6.6278\n",
      "Epoch 354/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8989 - val_loss: 6.6521\n",
      "Epoch 355/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9617 - val_loss: 6.6597\n",
      "Epoch 356/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9034 - val_loss: 6.6034\n",
      "Epoch 357/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8673 - val_loss: 6.6745\n",
      "Epoch 358/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9436 - val_loss: 6.5999\n",
      "Epoch 359/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9210 - val_loss: 6.6533\n",
      "Epoch 360/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9462 - val_loss: 6.6229\n",
      "Epoch 361/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9314 - val_loss: 6.7205\n",
      "Epoch 362/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8548 - val_loss: 6.7320\n",
      "Epoch 363/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9102 - val_loss: 6.7118\n",
      "Epoch 364/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8963 - val_loss: 6.6956\n",
      "Epoch 365/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8539 - val_loss: 6.7075\n",
      "Epoch 366/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8710 - val_loss: 6.6412\n",
      "Epoch 367/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8909 - val_loss: 6.6008\n",
      "Epoch 368/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8446 - val_loss: 6.5965\n",
      "Epoch 369/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8543 - val_loss: 6.6710\n",
      "Epoch 370/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8885 - val_loss: 6.6190\n",
      "Epoch 371/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8586 - val_loss: 6.6175\n",
      "Epoch 372/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8497 - val_loss: 6.6572\n",
      "Epoch 373/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9093 - val_loss: 6.5863\n",
      "Epoch 374/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8793 - val_loss: 6.6250\n",
      "Epoch 375/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8679 - val_loss: 6.6576\n",
      "Epoch 376/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8729 - val_loss: 6.6222\n",
      "Epoch 377/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8828 - val_loss: 6.7588\n",
      "Epoch 378/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8663 - val_loss: 6.6160\n",
      "Epoch 379/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8731 - val_loss: 6.6258\n",
      "Epoch 380/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9009 - val_loss: 6.6592\n",
      "Epoch 381/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8793 - val_loss: 6.6018\n",
      "Epoch 382/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8874 - val_loss: 6.6133\n",
      "Epoch 383/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8645 - val_loss: 6.6299\n",
      "Epoch 384/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9706 - val_loss: 6.6248\n",
      "Epoch 385/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9824 - val_loss: 6.6828\n",
      "Epoch 386/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9857 - val_loss: 6.6380\n",
      "Epoch 387/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9911 - val_loss: 6.6353\n",
      "Epoch 388/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8500 - val_loss: 6.6528\n",
      "Epoch 389/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8247 - val_loss: 6.7010\n",
      "Epoch 390/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9817 - val_loss: 6.6866\n",
      "Epoch 391/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9456 - val_loss: 6.6279\n",
      "Epoch 392/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9326 - val_loss: 6.6070\n",
      "Epoch 393/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0956 - val_loss: 6.6725\n",
      "Epoch 394/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.7957 - val_loss: 6.6537\n",
      "Epoch 395/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8889 - val_loss: 6.6021\n",
      "Epoch 396/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9010 - val_loss: 6.6492\n",
      "Epoch 397/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8501 - val_loss: 6.6198\n",
      "Epoch 398/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8731 - val_loss: 6.6274\n",
      "Epoch 399/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8266 - val_loss: 6.6472\n",
      "Epoch 400/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9028 - val_loss: 6.6211\n",
      "Epoch 401/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8925 - val_loss: 6.6325\n",
      "Epoch 402/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8805 - val_loss: 6.6768\n",
      "Epoch 403/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8725 - val_loss: 6.6363\n",
      "Epoch 404/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8306 - val_loss: 6.6039\n",
      "Epoch 405/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8828 - val_loss: 6.6275\n",
      "Epoch 406/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9058 - val_loss: 6.6864\n",
      "Epoch 407/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9170 - val_loss: 6.6428\n",
      "Epoch 408/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8049 - val_loss: 6.7080\n",
      "Epoch 409/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8558 - val_loss: 6.7539\n",
      "Epoch 410/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9156 - val_loss: 6.6444\n",
      "Epoch 411/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8648 - val_loss: 6.5934\n",
      "Epoch 412/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8788 - val_loss: 6.6755\n",
      "Epoch 413/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8770 - val_loss: 6.6805\n",
      "Epoch 414/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9524 - val_loss: 6.6960\n",
      "Epoch 415/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8888 - val_loss: 6.6016\n",
      "Epoch 416/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8824 - val_loss: 6.5975\n",
      "Epoch 417/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8714 - val_loss: 6.6265\n",
      "Epoch 418/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9066 - val_loss: 6.6681\n",
      "Epoch 419/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9933 - val_loss: 6.6175\n",
      "Epoch 420/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9172 - val_loss: 6.6235\n",
      "Epoch 421/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8338 - val_loss: 6.6494\n",
      "Epoch 422/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9141 - val_loss: 6.5985\n",
      "Epoch 423/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9785 - val_loss: 6.6860\n",
      "Epoch 424/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9059 - val_loss: 6.6333\n",
      "Epoch 425/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9010 - val_loss: 6.6266\n",
      "Epoch 426/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8577 - val_loss: 6.5951\n",
      "Epoch 427/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.7812 - val_loss: 6.5986\n",
      "Epoch 428/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8801 - val_loss: 6.6443\n",
      "Epoch 429/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8724 - val_loss: 6.6287\n",
      "Epoch 430/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8767 - val_loss: 6.6586\n",
      "Epoch 431/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9072 - val_loss: 6.6790\n",
      "Epoch 432/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8507 - val_loss: 6.5941\n",
      "Epoch 433/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8337 - val_loss: 6.6804\n",
      "Epoch 434/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8586 - val_loss: 6.6782\n",
      "Epoch 435/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9269 - val_loss: 6.6150\n",
      "Epoch 436/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 7.0416 - val_loss: 6.5713\n",
      "Epoch 437/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9439 - val_loss: 6.6026\n",
      "Epoch 438/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8851 - val_loss: 6.6304\n",
      "Epoch 439/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8255 - val_loss: 6.6956\n",
      "Epoch 440/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8742 - val_loss: 6.5942\n",
      "Epoch 441/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8730 - val_loss: 6.6437\n",
      "Epoch 442/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.7984 - val_loss: 6.6490\n",
      "Epoch 443/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9759 - val_loss: 6.6415\n",
      "Epoch 444/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8665 - val_loss: 6.6109\n",
      "Epoch 445/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8328 - val_loss: 6.6084\n",
      "Epoch 446/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9101 - val_loss: 6.5736\n",
      "Epoch 447/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8430 - val_loss: 6.6469\n",
      "Epoch 448/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.8646 - val_loss: 6.6086\n",
      "Epoch 449/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9404 - val_loss: 6.5592\n",
      "Epoch 450/450\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 6.9655 - val_loss: 6.6141\n"
     ]
    }
   ],
   "source": [
    "history = vae.fit(X_train, X_train, epochs=450, batch_size=256, shuffle=True, validation_data=(X_test, X_test))\n",
    "#vae.fit(train, epochs=epochs, batch_size=bs, shuffle=True, validation_data=(train, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save_weights('./vae_cncvae.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
